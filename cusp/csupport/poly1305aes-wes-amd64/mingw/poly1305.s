.text
.p2align 5
.globl _poly1305_amd64
.globl poly1305_amd64
_poly1305_amd64:
poly1305_amd64:
mov %rsp,%r11
lea poly1305_amd64_constants(%rip),%r10
sub %r10,%r11
and $4095,%r11
add $192,%r11
sub %r11,%rsp
movq %r11,32(%rsp)
movq %r12,40(%rsp)
movq %r13,48(%rsp)
movq %r14,56(%rsp)
movq %r15,64(%rsp)
movq %rdi,72(%rsp)
movq %rsi,80(%rsp)
movq %rbp,88(%rsp)
movq %rbx,96(%rsp)
mov  %rcx,%rax
fldcw poly1305_amd64_rounding(%rip)
mov  %rdx,%rcx
fildl 0(%rcx)
fstpl 104(%rsp)
fildl 4(%rcx)
fmull poly1305_amd64_two32(%rip)
fstl 112(%rsp)
fmull poly1305_amd64_scale(%rip)
fstpl 120(%rsp)
fildl 8(%rcx)
fmull poly1305_amd64_two64(%rip)
fstl 128(%rsp)
fmull poly1305_amd64_scale(%rip)
fstpl 136(%rsp)
fildl 12(%rcx)
fmull poly1305_amd64_two96(%rip)
fstl 144(%rsp)
fmull poly1305_amd64_scale(%rip)
fstpl 152(%rsp)
fldz
fldz
fldz
fldz
movl  $0x43300000,164(%rsp)
movl  $0x45300000,172(%rsp)
movl  $0x47300000,180(%rsp)
movl  $0x49300000,188(%rsp)
mov  %r9,%rsi
movq 40(%rsp,%r11),%rcx
cmp  $16,%rcx
jb ._addatmost15bytes
movl   12(%rsi),%edx
movl   8(%rsi),%r9d
movl   4(%rsi),%r10d
movl   0(%rsi),%r11d
movl %edx,184(%rsp)
movl %r9d,176(%rsp)
movl %r10d,168(%rsp)
movl %r11d,160(%rsp)
add  $16,%rsi
sub  $16,%rcx
fxch %st(3)
faddl 184(%rsp)
fsubl poly1305_amd64_doffset3minustwo128(%rip)
fxch %st(2)
faddl 176(%rsp)
fsubl poly1305_amd64_doffset2(%rip)
fxch %st(1)
faddl 168(%rsp)
fsubl poly1305_amd64_doffset1(%rip)
fxch %st(3)
faddl 160(%rsp)
fsubl poly1305_amd64_doffset0(%rip)
cmp  $16,%rcx
jb ._multiplyaddatmost15bytes
._multiplyaddatleast16bytes:
movl   12(%rsi),%edx
movl   8(%rsi),%r9d
movl   4(%rsi),%r10d
movl   0(%rsi),%r11d
movl %edx,184(%rsp)
movl %r9d,176(%rsp)
movl %r10d,168(%rsp)
movl %r11d,160(%rsp)
add  $16,%rsi
sub  $16,%rcx
fldl poly1305_amd64_alpha130(%rip)
fadd %st(3),%st(0)
fsubl poly1305_amd64_alpha130(%rip)
fsubr %st(0),%st(3)
fmull poly1305_amd64_scale(%rip)
fldl poly1305_amd64_alpha32(%rip)
fadd %st(2),%st(0)
fsubl poly1305_amd64_alpha32(%rip)
fsubr %st(0),%st(2)
fxch %st(2)
faddp %st(0),%st(1)
fldl poly1305_amd64_alpha64(%rip)
fadd %st(5),%st(0)
fsubl poly1305_amd64_alpha64(%rip)
fsubr %st(0),%st(5)
fldl poly1305_amd64_alpha96(%rip)
fadd %st(4),%st(0)
fsubl poly1305_amd64_alpha96(%rip)
fsubr %st(0),%st(4)
fxch %st(4)
faddp %st(0),%st(1)
fxch %st(4)
faddp %st(0),%st(3)
fxch %st(4)
faddp %st(0),%st(1)
fldl 144(%rsp)
fmul %st(4),%st(0)
fldl 128(%rsp)
fmul %st(5),%st(0)
fldl 112(%rsp)
fmul %st(6),%st(0)
fldl 104(%rsp)
fmulp %st(0),%st(7)
fldl 128(%rsp)
fmul %st(4),%st(0)
faddp %st(0),%st(3)
fldl 112(%rsp)
fmul %st(4),%st(0)
faddp %st(0),%st(2)
fldl 104(%rsp)
fmul %st(4),%st(0)
faddp %st(0),%st(1)
fldl 152(%rsp)
fmulp %st(0),%st(4)
fxch %st(3)
faddp %st(0),%st(6)
fldl 112(%rsp)
fmul %st(5),%st(0)
faddp %st(0),%st(2)
fldl 104(%rsp)
fmul %st(5),%st(0)
faddp %st(0),%st(1)
fldl 152(%rsp)
fmul %st(5),%st(0)
faddp %st(0),%st(3)
fldl 136(%rsp)
fmulp %st(0),%st(5)
fxch %st(4)
faddp %st(0),%st(5)
fldl 104(%rsp)
fmul %st(3),%st(0)
faddp %st(0),%st(1)
fldl 152(%rsp)
fmul %st(3),%st(0)
faddp %st(0),%st(4)
fldl 136(%rsp)
fmul %st(3),%st(0)
faddp %st(0),%st(2)
fxch %st(3)
fldl 120(%rsp)
fmulp %st(0),%st(3)
fxch %st(2)
faddp %st(0),%st(4)
cmp  $16,%rcx
fldl 184(%rsp)
fsubl poly1305_amd64_doffset3minustwo128(%rip)
faddp %st(0),%st(3)
fldl 176(%rsp)
fsubl poly1305_amd64_doffset2(%rip)
faddp %st(0),%st(2)
fldl 168(%rsp)
fsubl poly1305_amd64_doffset1(%rip)
faddp %st(0),%st(1)
fxch %st(3)
fldl 160(%rsp)
fsubl poly1305_amd64_doffset0(%rip)
faddp %st(0),%st(1)
jae ._multiplyaddatleast16bytes
._multiplyaddatmost15bytes:
fldl poly1305_amd64_alpha130(%rip)
fadd %st(3),%st(0)
fsubl poly1305_amd64_alpha130(%rip)
fsubr %st(0),%st(3)
fmull poly1305_amd64_scale(%rip)
fldl poly1305_amd64_alpha32(%rip)
fadd %st(2),%st(0)
fsubl poly1305_amd64_alpha32(%rip)
fsubr %st(0),%st(2)
fldl poly1305_amd64_alpha64(%rip)
fadd %st(6),%st(0)
fsubl poly1305_amd64_alpha64(%rip)
fsubr %st(0),%st(6)
fldl poly1305_amd64_alpha96(%rip)
fadd %st(5),%st(0)
fsubl poly1305_amd64_alpha96(%rip)
fsubr %st(0),%st(5)
fxch %st(4)
faddp %st(0),%st(3)
fxch %st(6)
faddp %st(0),%st(1)
fxch %st(3)
faddp %st(0),%st(5)
fxch %st(3)
faddp %st(0),%st(1)
fldl 144(%rsp)
fmul %st(3),%st(0)
fldl 128(%rsp)
fmul %st(4),%st(0)
fldl 112(%rsp)
fmul %st(5),%st(0)
fldl 104(%rsp)
fmulp %st(0),%st(6)
fldl 128(%rsp)
fmul %st(5),%st(0)
faddp %st(0),%st(3)
fldl 112(%rsp)
fmul %st(5),%st(0)
faddp %st(0),%st(2)
fldl 104(%rsp)
fmul %st(5),%st(0)
faddp %st(0),%st(1)
fldl 152(%rsp)
fmulp %st(0),%st(5)
fxch %st(4)
faddp %st(0),%st(5)
fldl 112(%rsp)
fmul %st(6),%st(0)
faddp %st(0),%st(2)
fldl 104(%rsp)
fmul %st(6),%st(0)
faddp %st(0),%st(1)
fldl 152(%rsp)
fmul %st(6),%st(0)
faddp %st(0),%st(4)
fldl 136(%rsp)
fmulp %st(0),%st(6)
fxch %st(5)
faddp %st(0),%st(4)
fldl 104(%rsp)
fmul %st(2),%st(0)
faddp %st(0),%st(1)
fldl 152(%rsp)
fmul %st(2),%st(0)
faddp %st(0),%st(5)
fldl 136(%rsp)
fmul %st(2),%st(0)
faddp %st(0),%st(3)
fldl 120(%rsp)
fmulp %st(0),%st(2)
fxch %st(1)
faddp %st(0),%st(3)
fxch %st(3)
fxch %st(2)
._addatmost15bytes:
cmp  $0,%rcx
je ._nomorebytes
movl $0,0(%rsp)
movl $0,4+0(%rsp)
movl $0,8+0(%rsp)
movl $0,12+0(%rsp)
leaq 0(%rsp),%rdi
rep movsb
movb   $1,0(%rdi)
movl 12+0(%rsp),%ecx
movl 8+0(%rsp),%edx
movl 4+0(%rsp),%r9d
movl 0(%rsp),%r10d
movl %ecx,184(%rsp)
movl %edx,176(%rsp)
movl %r9d,168(%rsp)
movl %r10d,160(%rsp)
fxch %st(3)
faddl 184(%rsp)
fsubl poly1305_amd64_doffset3(%rip)
fxch %st(2)
faddl 176(%rsp)
fsubl poly1305_amd64_doffset2(%rip)
fxch %st(1)
faddl 168(%rsp)
fsubl poly1305_amd64_doffset1(%rip)
fxch %st(3)
faddl 160(%rsp)
fsubl poly1305_amd64_doffset0(%rip)
fldl poly1305_amd64_alpha130(%rip)
fadd %st(3),%st(0)
fsubl poly1305_amd64_alpha130(%rip)
fsubr %st(0),%st(3)
fmull poly1305_amd64_scale(%rip)
fldl poly1305_amd64_alpha32(%rip)
fadd %st(2),%st(0)
fsubl poly1305_amd64_alpha32(%rip)
fsubr %st(0),%st(2)
fldl poly1305_amd64_alpha64(%rip)
fadd %st(6),%st(0)
fsubl poly1305_amd64_alpha64(%rip)
fsubr %st(0),%st(6)
fldl poly1305_amd64_alpha96(%rip)
fadd %st(5),%st(0)
fsubl poly1305_amd64_alpha96(%rip)
fsubr %st(0),%st(5)
fxch %st(4)
faddp %st(0),%st(3)
fxch %st(6)
faddp %st(0),%st(1)
fxch %st(3)
faddp %st(0),%st(5)
fxch %st(3)
faddp %st(0),%st(1)
fldl 144(%rsp)
fmul %st(3),%st(0)
fldl 128(%rsp)
fmul %st(4),%st(0)
fldl 112(%rsp)
fmul %st(5),%st(0)
fldl 104(%rsp)
fmulp %st(0),%st(6)
fldl 128(%rsp)
fmul %st(5),%st(0)
faddp %st(0),%st(3)
fldl 112(%rsp)
fmul %st(5),%st(0)
faddp %st(0),%st(2)
fldl 104(%rsp)
fmul %st(5),%st(0)
faddp %st(0),%st(1)
fldl 152(%rsp)
fmulp %st(0),%st(5)
fxch %st(4)
faddp %st(0),%st(5)
fldl 112(%rsp)
fmul %st(6),%st(0)
faddp %st(0),%st(2)
fldl 104(%rsp)
fmul %st(6),%st(0)
faddp %st(0),%st(1)
fldl 152(%rsp)
fmul %st(6),%st(0)
faddp %st(0),%st(4)
fldl 136(%rsp)
fmulp %st(0),%st(6)
fxch %st(5)
faddp %st(0),%st(4)
fldl 104(%rsp)
fmul %st(2),%st(0)
faddp %st(0),%st(1)
fldl 152(%rsp)
fmul %st(2),%st(0)
faddp %st(0),%st(5)
fldl 136(%rsp)
fmul %st(2),%st(0)
faddp %st(0),%st(3)
fldl 120(%rsp)
fmulp %st(0),%st(2)
fxch %st(1)
faddp %st(0),%st(3)
fxch %st(3)
fxch %st(2)
._nomorebytes:
fldl poly1305_amd64_alpha130(%rip)
fadd %st(4),%st(0)
fsubl poly1305_amd64_alpha130(%rip)
fsubr %st(0),%st(4)
fmull poly1305_amd64_scale(%rip)
fldl poly1305_amd64_alpha32(%rip)
fadd %st(2),%st(0)
fsubl poly1305_amd64_alpha32(%rip)
fsubr %st(0),%st(2)
fldl poly1305_amd64_alpha64(%rip)
fadd %st(4),%st(0)
fsubl poly1305_amd64_alpha64(%rip)
fsubr %st(0),%st(4)
fldl poly1305_amd64_alpha96(%rip)
fadd %st(6),%st(0)
fsubl poly1305_amd64_alpha96(%rip)
fsubr %st(0),%st(6)
fxch %st(4)
faddp %st(0),%st(3)
fxch %st(4)
faddp %st(0),%st(1)
fxch %st(4)
faddp %st(0),%st(3)
fxch %st(4)
faddp %st(0),%st(1)
fxch %st(3)
faddl poly1305_amd64_hoffset0(%rip)
fxch %st(2)
faddl poly1305_amd64_hoffset1(%rip)
fxch %st(1)
faddl poly1305_amd64_hoffset2(%rip)
fxch %st(3)
faddl poly1305_amd64_hoffset3(%rip)
fxch %st(2)
fstpl 104(%rsp)
fstpl 112(%rsp)
fxch %st(1)
fstpl 120(%rsp)
fstpl 128(%rsp)
movl 108(%rsp),%ecx
and  $63,%ecx
movl 116(%rsp),%edx
and  $63,%edx
movl 124(%rsp),%r9d
and  $63,%r9d
movl 132(%rsp),%r10d
and  $63,%r10d
movl 112(%rsp),%r11d
add %ecx,%r11d
movl 120(%rsp),%ecx
adc %edx,%ecx
movl 128(%rsp),%edx
adc %r9d,%edx
mov  $0,%r9
adc %r10d,%r9d
mov  $5,%r10
movl 104(%rsp),%r12d
add %r12d,%r10d
mov  $0,%r13
adc %r11d,%r13d
mov  $0,%r14
adc %ecx,%r14d
mov  $0,%r15
adc %edx,%r15d
mov  $-4,%rdi
adc %r9d,%edi
sar  $16,%edi
mov  %rdi,%r9
xor  $-1,%r9d
and  %rdi,%r12
and  %r9,%r10
or   %r10,%r12
and  %rdi,%r11
and  %r9,%r13
or   %r13,%r11
and  %rdi,%rcx
and  %r9,%r14
or   %r14,%rcx
and  %rdi,%rdx
and  %r9,%r15
or   %r15,%rdx
mov  %r8,%r8
addl 0(%r8),%r12d
adcl 4(%r8),%r11d
adcl 8(%r8),%ecx
adcl 12(%r8),%edx
mov  %rax,%r8
movl   %r12d,0(%r8)
movl   %r11d,4(%r8)
movl   %ecx,8(%r8)
movl   %edx,12(%r8)
movq 32(%rsp),%r11
movq 40(%rsp),%r12
movq 48(%rsp),%r13
movq 56(%rsp),%r14
movq 64(%rsp),%r15
movq 72(%rsp),%rdi
movq 80(%rsp),%rsi
movq 88(%rsp),%rbp
movq 96(%rsp),%rbx
add %r11,%rsp
ret
