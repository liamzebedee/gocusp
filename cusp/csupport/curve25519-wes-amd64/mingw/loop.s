.text
.p2align 5
.globl _curve25519_wes_loop
.globl curve25519_wes_loop
_curve25519_wes_loop:
curve25519_wes_loop:
mov %rsp,%r11
and $31,%r11
add $288,%r11
sub %r11,%rsp
movdqa %xmm6,0(%rsp)
movdqa %xmm7,16(%rsp)
movdqa %xmm8,32(%rsp)
movdqa %xmm9,48(%rsp)
movdqa %xmm10,64(%rsp)
movdqa %xmm11,80(%rsp)
movdqa %xmm12,96(%rsp)
movdqa %xmm13,112(%rsp)
movdqa %xmm14,128(%rsp)
movdqa %xmm15,144(%rsp)
movq %r11,160(%rsp)
movq %r12,168(%rsp)
movq %r13,176(%rsp)
movq %r14,184(%rsp)
movq %r15,192(%rsp)
movq %rdi,200(%rsp)
movq %rsi,208(%rsp)
movq %rbp,216(%rsp)
movq %rbx,224(%rsp)
movq %rcx,232(%rsp)
movq 0(%rcx),%mm0
movq 8(%rcx),%mm1
movq 16(%rcx),%mm2
movq 24(%rcx),%mm3
movq 32(%rcx),%mm4
mov  $1,%rcx
pxor   %mm5,%mm5
movd   %rcx,%xmm15
movq2dq %mm0,%xmm0
movq2dq %mm1,%xmm1
movq2dq %mm2,%xmm2
movq2dq %mm3,%xmm3
movq2dq %mm4,%xmm4
movdqa %xmm15,%xmm5
pxor   %xmm6,%xmm6
pxor   %xmm7,%xmm7
pxor   %xmm8,%xmm8
pxor   %xmm9,%xmm9
punpcklqdq %xmm0,%xmm5
punpcklqdq %xmm1,%xmm6
punpcklqdq %xmm2,%xmm7
punpcklqdq %xmm3,%xmm8
punpcklqdq %xmm4,%xmm9
movdqa %xmm15,%xmm10
pxor   %xmm11,%xmm11
pxor   %xmm12,%xmm12
pxor   %xmm13,%xmm13
pxor   %xmm14,%xmm14
pshufd $0x4e,%xmm10,%xmm10
leaq 256(%rsp),%rdi
mov  %rdx,%rsi
mov  %r8,%rcx
rep movsb
mov  $0,%rax
mov  $32,%rcx
sub  %r8,%rcx
rep stosb
pxor   %mm6,%mm6
sub  $1,%r8
mov  $0xf8,%r9
and  %r8,%r9
add  $8,%r9
shl  $32,%r9
._quadloop:
shr  $32,%r9
je ._done
sub  $8,%r9
leaq 256(%rsp),%rcx
movq (%rcx,%r9),%mm5
shl  $32,%r9
add  $64,%r9
._bitloop:
cmp  $0,%r9d
je ._quadloop
sub  $1,%r9
movq %mm5,%mm7
psllq $1,%mm5
psrad $31,%mm7
punpckhdq %mm7,%mm7
pxor  %mm7,%mm6
movq2dq %mm6,%xmm15
punpcklqdq %xmm15,%xmm15
movq %mm7,%mm6
paddq %xmm5,%xmm10
paddq %xmm6,%xmm11
paddq %xmm7,%xmm12
paddq %xmm8,%xmm13
paddq %xmm9,%xmm14
psllq $1,%xmm5
psllq $1,%xmm6
psllq $1,%xmm7
psllq $1,%xmm8
psllq $1,%xmm9
psubq %xmm10,%xmm5
psubq %xmm11,%xmm6
psubq %xmm12,%xmm7
psubq %xmm13,%xmm8
psubq %xmm14,%xmm9
movdqa %xmm10,%xmm0
movdqa %xmm11,%xmm1
movdqa %xmm12,%xmm2
movdqa %xmm13,%xmm3
movdqa %xmm14,%xmm4
punpckhqdq %xmm5,%xmm0
punpckhqdq %xmm6,%xmm1
punpckhqdq %xmm7,%xmm2
punpckhqdq %xmm8,%xmm3
punpckhqdq %xmm9,%xmm4
punpcklqdq %xmm5,%xmm10
punpcklqdq %xmm6,%xmm11
punpcklqdq %xmm7,%xmm12
punpcklqdq %xmm8,%xmm13
punpcklqdq %xmm9,%xmm14
movdqa %xmm0,%xmm5
movdqa %xmm1,%xmm6
movdqa %xmm2,%xmm7
movdqa %xmm3,%xmm8
movdqa %xmm4,%xmm9
psubq %xmm10,%xmm5
psubq %xmm11,%xmm6
psubq %xmm12,%xmm7
psubq %xmm13,%xmm8
psubq %xmm14,%xmm9
pand  %xmm15,%xmm5
pand  %xmm15,%xmm6
pand  %xmm15,%xmm7
pand  %xmm15,%xmm8
pand  %xmm15,%xmm9
psubq %xmm5,%xmm0
psubq %xmm6,%xmm1
psubq %xmm7,%xmm2
psubq %xmm8,%xmm3
psubq %xmm9,%xmm4
paddq %xmm5,%xmm10
paddq %xmm6,%xmm11
paddq %xmm7,%xmm12
paddq %xmm8,%xmm13
paddq %xmm9,%xmm14
pshufd $0x4e,%xmm10,%xmm5
pshufd $0x4e,%xmm11,%xmm6
pshufd $0x4e,%xmm12,%xmm7
pshufd $0x4e,%xmm13,%xmm8
pshufd $0x4e,%xmm14,%xmm9
movd   %xmm5,%r10
movd   %xmm6,%rcx
movd   %xmm7,%r12
movd   %xmm8,%r13
movd   %xmm9,%r14
call mulACtoB
movd   %r15,%xmm5
movd   %rdi,%xmm6
movd   %rsi,%xmm7
movd   %rbp,%xmm8
movd   %rbx,%xmm9
movd   %xmm10,%r10
movd   %xmm11,%rcx
movd   %xmm12,%r12
movd   %xmm13,%r13
movd   %xmm14,%r14
pshufd $0x4e,%xmm0,%xmm0
pshufd $0x4e,%xmm1,%xmm1
pshufd $0x4e,%xmm2,%xmm2
pshufd $0x4e,%xmm3,%xmm3
pshufd $0x4e,%xmm4,%xmm4
call mulACtoB
movd   %xmm5,%rcx
movd   %xmm6,%rdx
movd   %xmm7,%r8
movd   %xmm8,%rax
movd   %xmm9,%r10
add  %rcx,%r15
add  %rdx,%rdi
add  %r8,%rsi
add  %rax,%rbp
add  %r10,%rbx
shl  $1,%rcx
shl  $1,%rdx
shl  $1,%r8
shl  $1,%rax
shl  $1,%r10
sub  %r15,%rcx
sub  %rdi,%rdx
sub  %rsi,%r8
sub  %rbp,%rax
sub  %rbx,%r10
movd   %rcx,%xmm0
movd   %rdx,%xmm1
movd   %r8,%xmm2
movd   %rax,%xmm3
movd   %r10,%xmm4
call sqrBtoA
movd   %r10,%xmm5
movd   %rcx,%xmm6
movd   %r12,%xmm7
movd   %r13,%xmm8
movd   %r14,%xmm9
movd   %xmm0,%r15
movd   %xmm1,%rdi
movd   %xmm2,%rsi
movd   %xmm3,%rbp
movd   %xmm4,%rbx
movq2dq %mm0,%xmm0
movq2dq %mm1,%xmm1
movq2dq %mm2,%xmm2
movq2dq %mm3,%xmm3
movq2dq %mm4,%xmm4
call sqrBtoA
call mulACtoB
movd   %r15,%xmm0
movd   %rdi,%xmm1
movd   %rsi,%xmm2
movd   %rbp,%xmm3
movd   %rbx,%xmm4
punpcklqdq %xmm0,%xmm5
punpcklqdq %xmm1,%xmm6
punpcklqdq %xmm2,%xmm7
punpcklqdq %xmm3,%xmm8
punpcklqdq %xmm4,%xmm9
movd   %xmm10,%r15
movd   %xmm11,%rdi
movd   %xmm12,%rsi
movd   %xmm13,%rbp
movd   %xmm14,%rbx
call sqrBtoA
movd   %r10,%xmm0
movd   %rcx,%xmm1
movd   %r12,%xmm2
movd   %r13,%xmm3
movd   %r14,%xmm4
pshufd $0x4e,%xmm10,%xmm10
pshufd $0x4e,%xmm11,%xmm11
pshufd $0x4e,%xmm12,%xmm12
pshufd $0x4e,%xmm13,%xmm13
pshufd $0x4e,%xmm14,%xmm14
movd   %xmm10,%r15
movd   %xmm11,%rdi
movd   %xmm12,%rsi
movd   %xmm13,%rbp
movd   %xmm14,%rbx
call sqrBtoA
call mulACtoB
movd   %r15,%xmm10
movd   %rdi,%xmm11
movd   %rsi,%xmm12
movd   %rbp,%xmm13
movd   %rbx,%xmm14
movd   %xmm0,%r15
movd   %xmm1,%rdi
movd   %xmm2,%rsi
movd   %xmm3,%rbp
movd   %xmm4,%rbx
sub  %r10,%r15
sub  %rcx,%rdi
sub  %r12,%rsi
sub  %r13,%rbp
sub  %r14,%rbx
call scalBtoA
movd   %xmm0,%rdx
movd   %xmm1,%r8
movd   %xmm2,%rax
add  %rdx,%r10
add  %r8,%rcx
add  %rax,%r12
movd   %r15,%xmm0
movd   %rdi,%xmm1
movd   %rsi,%xmm2
movd   %xmm3,%rdx
movd   %xmm4,%r8
add  %rdx,%r13
add  %r8,%r14
movd   %rbp,%xmm3
movd   %rbx,%xmm4
call mulACtoB
movd   %r15,%xmm0
movd   %rdi,%xmm1
movd   %rsi,%xmm2
movd   %rbp,%xmm3
movd   %rbx,%xmm4
punpcklqdq %xmm5,%xmm10
punpcklqdq %xmm6,%xmm11
punpcklqdq %xmm7,%xmm12
punpcklqdq %xmm8,%xmm13
punpcklqdq %xmm9,%xmm14
pshufd $0x4e,%xmm5,%xmm5
pshufd $0x4e,%xmm6,%xmm6
pshufd $0x4e,%xmm7,%xmm7
pshufd $0x4e,%xmm8,%xmm8
pshufd $0x4e,%xmm9,%xmm9
punpcklqdq %xmm5,%xmm0
punpcklqdq %xmm6,%xmm1
punpcklqdq %xmm7,%xmm2
punpcklqdq %xmm8,%xmm3
punpcklqdq %xmm9,%xmm4
movdqa %xmm10,%xmm5
movdqa %xmm11,%xmm6
movdqa %xmm12,%xmm7
movdqa %xmm13,%xmm8
movdqa %xmm14,%xmm9
movdqa %xmm0,%xmm10
movdqa %xmm1,%xmm11
movdqa %xmm2,%xmm12
movdqa %xmm3,%xmm13
movdqa %xmm4,%xmm14
jmp ._bitloop
._done:
movdqa %xmm10,%xmm0
movdqa %xmm11,%xmm1
movdqa %xmm12,%xmm2
movdqa %xmm13,%xmm3
movdqa %xmm14,%xmm4
punpckhqdq %xmm5,%xmm0
punpckhqdq %xmm6,%xmm1
punpckhqdq %xmm7,%xmm2
punpckhqdq %xmm8,%xmm3
punpckhqdq %xmm9,%xmm4
punpcklqdq %xmm5,%xmm10
punpcklqdq %xmm6,%xmm11
punpcklqdq %xmm7,%xmm12
punpcklqdq %xmm8,%xmm13
punpcklqdq %xmm9,%xmm14
movq2dq %mm6,%xmm15
punpcklqdq %xmm15,%xmm15
movdqa %xmm0,%xmm5
movdqa %xmm1,%xmm6
movdqa %xmm2,%xmm7
movdqa %xmm3,%xmm8
movdqa %xmm4,%xmm9
psubq %xmm10,%xmm5
psubq %xmm11,%xmm6
psubq %xmm12,%xmm7
psubq %xmm13,%xmm8
psubq %xmm14,%xmm9
pand  %xmm15,%xmm5
pand  %xmm15,%xmm6
pand  %xmm15,%xmm7
pand  %xmm15,%xmm8
pand  %xmm15,%xmm9
psubq %xmm5,%xmm0
psubq %xmm6,%xmm1
psubq %xmm7,%xmm2
psubq %xmm8,%xmm3
psubq %xmm9,%xmm4
paddq %xmm5,%xmm10
paddq %xmm6,%xmm11
paddq %xmm7,%xmm12
paddq %xmm8,%xmm13
paddq %xmm9,%xmm14
movq 232(%rsp),%rcx
movd   %xmm10,%rdx
movd   %xmm11,%r8
movd   %xmm12,%r9
movd   %xmm13,%rax
movd   %xmm14,%r10
pshufd $0x4e,%xmm10,%xmm10
pshufd $0x4e,%xmm11,%xmm11
pshufd $0x4e,%xmm12,%xmm12
pshufd $0x4e,%xmm13,%xmm13
pshufd $0x4e,%xmm14,%xmm14
movd   %xmm10,%r11
movd   %xmm11,%r12
movd   %xmm12,%r13
movd   %xmm13,%r14
movd   %xmm14,%r15
movq   %r11,0(%rcx)
movq   %r12,8(%rcx)
movq   %r13,16(%rcx)
movq   %r14,24(%rcx)
movq   %r15,32(%rcx)
movq   %rdx,40(%rcx)
movq   %r8,48(%rcx)
movq   %r9,56(%rcx)
movq   %rax,64(%rcx)
movq   %r10,72(%rcx)
movd   %xmm0,%rdx
movd   %xmm1,%r8
movd   %xmm2,%r9
movd   %xmm3,%rax
movd   %xmm4,%r10
pshufd $0x4e,%xmm0,%xmm0
pshufd $0x4e,%xmm1,%xmm1
pshufd $0x4e,%xmm2,%xmm2
pshufd $0x4e,%xmm3,%xmm3
pshufd $0x4e,%xmm4,%xmm4
movd   %xmm0,%r11
movd   %xmm1,%r12
movd   %xmm2,%r13
movd   %xmm3,%r14
movd   %xmm4,%r15
movq   %r11,80(%rcx)
movq   %r12,88(%rcx)
movq   %r13,96(%rcx)
movq   %r14,104(%rcx)
movq   %r15,112(%rcx)
movq   %rdx,120(%rcx)
movq   %r8,128(%rcx)
movq   %r9,136(%rcx)
movq   %rax,144(%rcx)
movq   %r10,152(%rcx)
movdqa 0(%rsp),%xmm6
movdqa 16(%rsp),%xmm7
movdqa 32(%rsp),%xmm8
movdqa 48(%rsp),%xmm9
movdqa 64(%rsp),%xmm10
movdqa 80(%rsp),%xmm11
movdqa 96(%rsp),%xmm12
movdqa 112(%rsp),%xmm13
movdqa 128(%rsp),%xmm14
movdqa 144(%rsp),%xmm15
movq 160(%rsp),%r11
movq 168(%rsp),%r12
movq 176(%rsp),%r13
movq 184(%rsp),%r14
movq 192(%rsp),%r15
movq 200(%rsp),%rdi
movq 208(%rsp),%rsi
movq 216(%rsp),%rbp
movq 224(%rsp),%rbx
emms
add %r11,%rsp
ret
