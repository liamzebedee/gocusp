.text
.p2align 5
.globl _aes128_x86mmx1_block
.globl aes128_x86mmx1_block
_aes128_x86mmx1_block:
aes128_x86mmx1_block:
mov %esp,%eax
sub $aes128_x86mmx1_constants,%eax
and $4095,%eax
add $224,%eax
sub %eax,%esp
movl %eax,0(%esp)
movl %ebx,4(%esp)
movl %esi,8(%esp)
movl %edi,12(%esp)
movl %ebp,16(%esp)
movl 12(%esp,%eax),%ecx
movl 4(%esp,%eax),%edx
movd %edx,%mm0
movl 0(%ecx),%edx
movl 4(%ecx),%ebx
movl 8(%ecx),%esi
movl 12(%ecx),%edi
movl %edx,20(%esp)
movl %ebx,24(%esp)
movl %esi,28(%esp)
movl %edi,32(%esp)
movl 16(%ecx),%edx
xorl %edx,%ebx
xorl %ebx,%esi
xorl %esi,%edi
movl %edx,36(%esp)
movl %ebx,40(%esp)
movl %esi,44(%esp)
movl %edi,48(%esp)
movl 20(%ecx),%edx
xorl %edx,%ebx
xorl %ebx,%esi
xorl %esi,%edi
movl %edx,52(%esp)
movl %ebx,56(%esp)
movl %esi,60(%esp)
movl %edi,64(%esp)
movl 24(%ecx),%edx
xorl %edx,%ebx
xorl %ebx,%esi
xorl %esi,%edi
movl %edx,68(%esp)
movl %ebx,72(%esp)
movl %esi,76(%esp)
movl %edi,80(%esp)
movl 28(%ecx),%edx
xorl %edx,%ebx
xorl %ebx,%esi
xorl %esi,%edi
movl %edx,84(%esp)
movl %ebx,88(%esp)
movl %esi,92(%esp)
movl %edi,96(%esp)
movl 32(%ecx),%edx
xorl %edx,%ebx
xorl %ebx,%esi
xorl %esi,%edi
movl %edx,100(%esp)
movl %ebx,104(%esp)
movl %esi,108(%esp)
movl %edi,112(%esp)
movl 36(%ecx),%edx
xorl %edx,%ebx
xorl %ebx,%esi
xorl %esi,%edi
movl %edx,116(%esp)
movl %ebx,120(%esp)
movl %esi,124(%esp)
movl %edi,128(%esp)
movl 40(%ecx),%edx
xorl %edx,%ebx
xorl %ebx,%esi
xorl %esi,%edi
movl %edx,132(%esp)
movl %ebx,136(%esp)
movl %esi,140(%esp)
movl %edi,144(%esp)
movl 44(%ecx),%edx
xorl %edx,%ebx
xorl %ebx,%esi
xorl %esi,%edi
movl %edx,148(%esp)
movl %ebx,152(%esp)
movl %esi,156(%esp)
movl %edi,160(%esp)
movl 48(%ecx),%edx
xorl %edx,%ebx
xorl %ebx,%esi
xorl %esi,%edi
movl %edx,164(%esp)
movl %ebx,168(%esp)
movl %esi,172(%esp)
movl %edi,176(%esp)
movl 52(%ecx),%ecx
xorl %ecx,%ebx
xorl %ebx,%esi
xorl %esi,%edi
movl %ecx,180(%esp)
movl %ebx,184(%esp)
movl %esi,188(%esp)
movl %edi,192(%esp)
movl 8(%esp,%eax),%ebx
movl 0(%ebx),%eax
movl 4(%ebx),%ecx
movl 8(%ebx),%edx
movl 12(%ebx),%ebx
xorl 20(%esp),%eax
xorl 24(%esp),%ecx
xorl 28(%esp),%edx
xorl 32(%esp),%ebx
movd %ebx,%mm1
movzbl %al,%ebx
movl aes128_x86mmx1_table0(,%ebx,8),%ebx
movzbl %ah,%esi
shr  $16,%eax
movl aes128_x86mmx1_table1(,%esi,8),%esi
movzbl %al,%edi
movl aes128_x86mmx1_table2(,%edi,8),%edi
movzbl %ah,%eax
movl aes128_x86mmx1_table3(,%eax,8),%ebp
movzbl %cl,%eax
xorl aes128_x86mmx1_table0(,%eax,8),%ebp
movzbl %ch,%eax
xorl aes128_x86mmx1_table1(,%eax,8),%ebx
shr  $16,%ecx
movzbl %cl,%eax
xorl aes128_x86mmx1_table2(,%eax,8),%esi
movzbl %ch,%eax
xorl aes128_x86mmx1_table3(,%eax,8),%edi
movd %mm1,%eax
movzbl %dl,%ecx
xorl aes128_x86mmx1_table0(,%ecx,8),%edi
movzbl %dh,%ecx
xorl aes128_x86mmx1_table1(,%ecx,8),%ebp
shr  $16,%edx
movzbl %dl,%ecx
xorl aes128_x86mmx1_table2(,%ecx,8),%ebx
movzbl %dh,%ecx
xorl aes128_x86mmx1_table3(,%ecx,8),%esi
movzbl %al,%ecx
xorl aes128_x86mmx1_table0(,%ecx,8),%esi
movzbl %ah,%ecx
xorl aes128_x86mmx1_table1(,%ecx,8),%edi
shr  $16,%eax
movzbl %al,%ecx
xorl aes128_x86mmx1_table2(,%ecx,8),%ebp
movzbl %ah,%eax
xorl aes128_x86mmx1_table3(,%eax,8),%ebx
movl 36(%esp),%eax
xorl %ebx,%eax
movl 40(%esp),%ecx
xorl %ebp,%ecx
movl 44(%esp),%edx
xorl %edi,%edx
movl 48(%esp),%ebx
xorl %esi,%ebx
movd %ebx,%mm1
movzbl %al,%ebx
movl aes128_x86mmx1_table0(,%ebx,8),%ebx
movzbl %ah,%esi
shr  $16,%eax
movl aes128_x86mmx1_table1(,%esi,8),%esi
movzbl %al,%edi
movl aes128_x86mmx1_table2(,%edi,8),%edi
movzbl %ah,%eax
movl aes128_x86mmx1_table3(,%eax,8),%ebp
movzbl %cl,%eax
xorl aes128_x86mmx1_table0(,%eax,8),%ebp
movzbl %ch,%eax
xorl aes128_x86mmx1_table1(,%eax,8),%ebx
shr  $16,%ecx
movzbl %cl,%eax
xorl aes128_x86mmx1_table2(,%eax,8),%esi
movzbl %ch,%eax
xorl aes128_x86mmx1_table3(,%eax,8),%edi
movd %mm1,%eax
movzbl %dl,%ecx
xorl aes128_x86mmx1_table0(,%ecx,8),%edi
movzbl %dh,%ecx
xorl aes128_x86mmx1_table1(,%ecx,8),%ebp
shr  $16,%edx
movzbl %dl,%ecx
xorl aes128_x86mmx1_table2(,%ecx,8),%ebx
movzbl %dh,%ecx
xorl aes128_x86mmx1_table3(,%ecx,8),%esi
movzbl %al,%ecx
xorl aes128_x86mmx1_table0(,%ecx,8),%esi
movzbl %ah,%ecx
xorl aes128_x86mmx1_table1(,%ecx,8),%edi
shr  $16,%eax
movzbl %al,%ecx
xorl aes128_x86mmx1_table2(,%ecx,8),%ebp
movzbl %ah,%eax
xorl aes128_x86mmx1_table3(,%eax,8),%ebx
movl 52(%esp),%eax
xorl %ebx,%eax
movl 56(%esp),%ecx
xorl %ebp,%ecx
movl 60(%esp),%edx
xorl %edi,%edx
movl 64(%esp),%ebx
xorl %esi,%ebx
movd %ebx,%mm1
movzbl %al,%ebx
movl aes128_x86mmx1_table0(,%ebx,8),%ebx
movzbl %ah,%esi
shr  $16,%eax
movl aes128_x86mmx1_table1(,%esi,8),%esi
movzbl %al,%edi
movl aes128_x86mmx1_table2(,%edi,8),%edi
movzbl %ah,%eax
movl aes128_x86mmx1_table3(,%eax,8),%ebp
movzbl %cl,%eax
xorl aes128_x86mmx1_table0(,%eax,8),%ebp
movzbl %ch,%eax
xorl aes128_x86mmx1_table1(,%eax,8),%ebx
shr  $16,%ecx
movzbl %cl,%eax
xorl aes128_x86mmx1_table2(,%eax,8),%esi
movzbl %ch,%eax
xorl aes128_x86mmx1_table3(,%eax,8),%edi
movd %mm1,%eax
movzbl %dl,%ecx
xorl aes128_x86mmx1_table0(,%ecx,8),%edi
movzbl %dh,%ecx
xorl aes128_x86mmx1_table1(,%ecx,8),%ebp
shr  $16,%edx
movzbl %dl,%ecx
xorl aes128_x86mmx1_table2(,%ecx,8),%ebx
movzbl %dh,%ecx
xorl aes128_x86mmx1_table3(,%ecx,8),%esi
movzbl %al,%ecx
xorl aes128_x86mmx1_table0(,%ecx,8),%esi
movzbl %ah,%ecx
xorl aes128_x86mmx1_table1(,%ecx,8),%edi
shr  $16,%eax
movzbl %al,%ecx
xorl aes128_x86mmx1_table2(,%ecx,8),%ebp
movzbl %ah,%eax
xorl aes128_x86mmx1_table3(,%eax,8),%ebx
movl 68(%esp),%eax
xorl %ebx,%eax
movl 72(%esp),%ecx
xorl %ebp,%ecx
movl 76(%esp),%edx
xorl %edi,%edx
movl 80(%esp),%ebx
xorl %esi,%ebx
movd %ebx,%mm1
movzbl %al,%ebx
movl aes128_x86mmx1_table0(,%ebx,8),%ebx
movzbl %ah,%esi
shr  $16,%eax
movl aes128_x86mmx1_table1(,%esi,8),%esi
movzbl %al,%edi
movl aes128_x86mmx1_table2(,%edi,8),%edi
movzbl %ah,%eax
movl aes128_x86mmx1_table3(,%eax,8),%ebp
movzbl %cl,%eax
xorl aes128_x86mmx1_table0(,%eax,8),%ebp
movzbl %ch,%eax
xorl aes128_x86mmx1_table1(,%eax,8),%ebx
shr  $16,%ecx
movzbl %cl,%eax
xorl aes128_x86mmx1_table2(,%eax,8),%esi
movzbl %ch,%eax
xorl aes128_x86mmx1_table3(,%eax,8),%edi
movd %mm1,%eax
movzbl %dl,%ecx
xorl aes128_x86mmx1_table0(,%ecx,8),%edi
movzbl %dh,%ecx
xorl aes128_x86mmx1_table1(,%ecx,8),%ebp
shr  $16,%edx
movzbl %dl,%ecx
xorl aes128_x86mmx1_table2(,%ecx,8),%ebx
movzbl %dh,%ecx
xorl aes128_x86mmx1_table3(,%ecx,8),%esi
movzbl %al,%ecx
xorl aes128_x86mmx1_table0(,%ecx,8),%esi
movzbl %ah,%ecx
xorl aes128_x86mmx1_table1(,%ecx,8),%edi
shr  $16,%eax
movzbl %al,%ecx
xorl aes128_x86mmx1_table2(,%ecx,8),%ebp
movzbl %ah,%eax
xorl aes128_x86mmx1_table3(,%eax,8),%ebx
movl 84(%esp),%eax
xorl %ebx,%eax
movl 88(%esp),%ecx
xorl %ebp,%ecx
movl 92(%esp),%edx
xorl %edi,%edx
movl 96(%esp),%ebx
xorl %esi,%ebx
movd %ebx,%mm1
movzbl %al,%ebx
movl aes128_x86mmx1_table0(,%ebx,8),%ebx
movzbl %ah,%esi
shr  $16,%eax
movl aes128_x86mmx1_table1(,%esi,8),%esi
movzbl %al,%edi
movl aes128_x86mmx1_table2(,%edi,8),%edi
movzbl %ah,%eax
movl aes128_x86mmx1_table3(,%eax,8),%ebp
movzbl %cl,%eax
xorl aes128_x86mmx1_table0(,%eax,8),%ebp
movzbl %ch,%eax
xorl aes128_x86mmx1_table1(,%eax,8),%ebx
shr  $16,%ecx
movzbl %cl,%eax
xorl aes128_x86mmx1_table2(,%eax,8),%esi
movzbl %ch,%eax
xorl aes128_x86mmx1_table3(,%eax,8),%edi
movd %mm1,%eax
movzbl %dl,%ecx
xorl aes128_x86mmx1_table0(,%ecx,8),%edi
movzbl %dh,%ecx
xorl aes128_x86mmx1_table1(,%ecx,8),%ebp
shr  $16,%edx
movzbl %dl,%ecx
xorl aes128_x86mmx1_table2(,%ecx,8),%ebx
movzbl %dh,%ecx
xorl aes128_x86mmx1_table3(,%ecx,8),%esi
movzbl %al,%ecx
xorl aes128_x86mmx1_table0(,%ecx,8),%esi
movzbl %ah,%ecx
xorl aes128_x86mmx1_table1(,%ecx,8),%edi
shr  $16,%eax
movzbl %al,%ecx
xorl aes128_x86mmx1_table2(,%ecx,8),%ebp
movzbl %ah,%eax
xorl aes128_x86mmx1_table3(,%eax,8),%ebx
movl 100(%esp),%eax
xorl %ebx,%eax
movl 104(%esp),%ecx
xorl %ebp,%ecx
movl 108(%esp),%edx
xorl %edi,%edx
movl 112(%esp),%ebx
xorl %esi,%ebx
movd %ebx,%mm1
movzbl %al,%ebx
movl aes128_x86mmx1_table0(,%ebx,8),%ebx
movzbl %ah,%esi
shr  $16,%eax
movl aes128_x86mmx1_table1(,%esi,8),%esi
movzbl %al,%edi
movl aes128_x86mmx1_table2(,%edi,8),%edi
movzbl %ah,%eax
movl aes128_x86mmx1_table3(,%eax,8),%ebp
movzbl %cl,%eax
xorl aes128_x86mmx1_table0(,%eax,8),%ebp
movzbl %ch,%eax
xorl aes128_x86mmx1_table1(,%eax,8),%ebx
shr  $16,%ecx
movzbl %cl,%eax
xorl aes128_x86mmx1_table2(,%eax,8),%esi
movzbl %ch,%eax
xorl aes128_x86mmx1_table3(,%eax,8),%edi
movd %mm1,%eax
movzbl %dl,%ecx
xorl aes128_x86mmx1_table0(,%ecx,8),%edi
movzbl %dh,%ecx
xorl aes128_x86mmx1_table1(,%ecx,8),%ebp
shr  $16,%edx
movzbl %dl,%ecx
xorl aes128_x86mmx1_table2(,%ecx,8),%ebx
movzbl %dh,%ecx
xorl aes128_x86mmx1_table3(,%ecx,8),%esi
movzbl %al,%ecx
xorl aes128_x86mmx1_table0(,%ecx,8),%esi
movzbl %ah,%ecx
xorl aes128_x86mmx1_table1(,%ecx,8),%edi
shr  $16,%eax
movzbl %al,%ecx
xorl aes128_x86mmx1_table2(,%ecx,8),%ebp
movzbl %ah,%eax
xorl aes128_x86mmx1_table3(,%eax,8),%ebx
movl 116(%esp),%eax
xorl %ebx,%eax
movl 120(%esp),%ecx
xorl %ebp,%ecx
movl 124(%esp),%edx
xorl %edi,%edx
movl 128(%esp),%ebx
xorl %esi,%ebx
movd %ebx,%mm1
movzbl %al,%ebx
movl aes128_x86mmx1_table0(,%ebx,8),%ebx
movzbl %ah,%esi
shr  $16,%eax
movl aes128_x86mmx1_table1(,%esi,8),%esi
movzbl %al,%edi
movl aes128_x86mmx1_table2(,%edi,8),%edi
movzbl %ah,%eax
movl aes128_x86mmx1_table3(,%eax,8),%ebp
movzbl %cl,%eax
xorl aes128_x86mmx1_table0(,%eax,8),%ebp
movzbl %ch,%eax
xorl aes128_x86mmx1_table1(,%eax,8),%ebx
shr  $16,%ecx
movzbl %cl,%eax
xorl aes128_x86mmx1_table2(,%eax,8),%esi
movzbl %ch,%eax
xorl aes128_x86mmx1_table3(,%eax,8),%edi
movd %mm1,%eax
movzbl %dl,%ecx
xorl aes128_x86mmx1_table0(,%ecx,8),%edi
movzbl %dh,%ecx
xorl aes128_x86mmx1_table1(,%ecx,8),%ebp
shr  $16,%edx
movzbl %dl,%ecx
xorl aes128_x86mmx1_table2(,%ecx,8),%ebx
movzbl %dh,%ecx
xorl aes128_x86mmx1_table3(,%ecx,8),%esi
movzbl %al,%ecx
xorl aes128_x86mmx1_table0(,%ecx,8),%esi
movzbl %ah,%ecx
xorl aes128_x86mmx1_table1(,%ecx,8),%edi
shr  $16,%eax
movzbl %al,%ecx
xorl aes128_x86mmx1_table2(,%ecx,8),%ebp
movzbl %ah,%eax
xorl aes128_x86mmx1_table3(,%eax,8),%ebx
movl 132(%esp),%eax
xorl %ebx,%eax
movl 136(%esp),%ecx
xorl %ebp,%ecx
movl 140(%esp),%edx
xorl %edi,%edx
movl 144(%esp),%ebx
xorl %esi,%ebx
movd %ebx,%mm1
movzbl %al,%ebx
movl aes128_x86mmx1_table0(,%ebx,8),%ebx
movzbl %ah,%esi
shr  $16,%eax
movl aes128_x86mmx1_table1(,%esi,8),%esi
movzbl %al,%edi
movl aes128_x86mmx1_table2(,%edi,8),%edi
movzbl %ah,%eax
movl aes128_x86mmx1_table3(,%eax,8),%ebp
movzbl %cl,%eax
xorl aes128_x86mmx1_table0(,%eax,8),%ebp
movzbl %ch,%eax
xorl aes128_x86mmx1_table1(,%eax,8),%ebx
shr  $16,%ecx
movzbl %cl,%eax
xorl aes128_x86mmx1_table2(,%eax,8),%esi
movzbl %ch,%eax
xorl aes128_x86mmx1_table3(,%eax,8),%edi
movd %mm1,%eax
movzbl %dl,%ecx
xorl aes128_x86mmx1_table0(,%ecx,8),%edi
movzbl %dh,%ecx
xorl aes128_x86mmx1_table1(,%ecx,8),%ebp
shr  $16,%edx
movzbl %dl,%ecx
xorl aes128_x86mmx1_table2(,%ecx,8),%ebx
movzbl %dh,%ecx
xorl aes128_x86mmx1_table3(,%ecx,8),%esi
movzbl %al,%ecx
xorl aes128_x86mmx1_table0(,%ecx,8),%esi
movzbl %ah,%ecx
xorl aes128_x86mmx1_table1(,%ecx,8),%edi
shr  $16,%eax
movzbl %al,%ecx
xorl aes128_x86mmx1_table2(,%ecx,8),%ebp
movzbl %ah,%eax
xorl aes128_x86mmx1_table3(,%eax,8),%ebx
movl 148(%esp),%eax
xorl %ebx,%eax
movl 152(%esp),%ecx
xorl %ebp,%ecx
movl 156(%esp),%edx
xorl %edi,%edx
movl 160(%esp),%ebx
xorl %esi,%ebx
movd %ebx,%mm1
movzbl %al,%ebx
movl aes128_x86mmx1_table0(,%ebx,8),%ebx
movzbl %ah,%esi
shr  $16,%eax
movl aes128_x86mmx1_table1(,%esi,8),%esi
movzbl %al,%edi
movl aes128_x86mmx1_table2(,%edi,8),%edi
movzbl %ah,%eax
movl aes128_x86mmx1_table3(,%eax,8),%ebp
movzbl %cl,%eax
xorl aes128_x86mmx1_table0(,%eax,8),%ebp
movzbl %ch,%eax
xorl aes128_x86mmx1_table1(,%eax,8),%ebx
shr  $16,%ecx
movzbl %cl,%eax
xorl aes128_x86mmx1_table2(,%eax,8),%esi
movzbl %ch,%eax
xorl aes128_x86mmx1_table3(,%eax,8),%edi
movd %mm1,%eax
movzbl %dl,%ecx
xorl aes128_x86mmx1_table0(,%ecx,8),%edi
movzbl %dh,%ecx
xorl aes128_x86mmx1_table1(,%ecx,8),%ebp
shr  $16,%edx
movzbl %dl,%ecx
xorl aes128_x86mmx1_table2(,%ecx,8),%ebx
movzbl %dh,%ecx
xorl aes128_x86mmx1_table3(,%ecx,8),%esi
movzbl %al,%ecx
xorl aes128_x86mmx1_table0(,%ecx,8),%esi
movzbl %ah,%ecx
xorl aes128_x86mmx1_table1(,%ecx,8),%edi
shr  $16,%eax
movzbl %al,%ecx
xorl aes128_x86mmx1_table2(,%ecx,8),%ebp
movzbl %ah,%eax
xorl aes128_x86mmx1_table3(,%eax,8),%ebx
movl 164(%esp),%eax
xorl %ebx,%eax
movl 168(%esp),%ecx
xorl %ebp,%ecx
movl 172(%esp),%edx
xorl %edi,%edx
movl 176(%esp),%ebx
xorl %esi,%ebx
movd %ebx,%mm1
movzbl %al,%ebx
movzbl aes128_x86mmx1_table2(,%ebx,8),%ebx
movzbl %ah,%esi
movzwl aes128_x86mmx1_tablex(,%esi,8),%esi
shr  $16,%eax
movzbl %al,%edi
movl aes128_x86mmx1_table0(,%edi,8),%edi
and  $0x00ff0000,%edi
movzbl %ah,%eax
movl aes128_x86mmx1_table1(,%eax,8),%ebp
and  $0xff000000,%ebp
xorl 180(%esp),%ebx
xorl 192(%esp),%esi
xorl 184(%esp),%ebp
xorl 188(%esp),%edi
movzbl %cl,%eax
movzbl aes128_x86mmx1_table2(,%eax,8),%eax
xorl %eax,%ebp
movzbl %ch,%eax
movzwl aes128_x86mmx1_tablex(,%eax,8),%eax
xorl %eax,%ebx
shr  $16,%ecx
movzbl %cl,%eax
movl aes128_x86mmx1_table0(,%eax,8),%eax
and  $0x00ff0000,%eax
xorl %eax,%esi
movzbl %ch,%eax
movl aes128_x86mmx1_table1(,%eax,8),%eax
and  $0xff000000,%eax
xorl %eax,%edi
movd %mm1,%eax
movzbl %dl,%ecx
movzbl aes128_x86mmx1_table2(,%ecx,8),%ecx
xorl %ecx,%edi
movzbl %dh,%ecx
movzwl aes128_x86mmx1_tablex(,%ecx,8),%ecx
xorl %ecx,%ebp
shr  $16,%edx
movzbl %dl,%ecx
movl aes128_x86mmx1_table0(,%ecx,8),%ecx
and  $0x00ff0000,%ecx
xorl %ecx,%ebx
movzbl %dh,%ecx
movl aes128_x86mmx1_table1(,%ecx,8),%ecx
and  $0xff000000,%ecx
xorl %ecx,%esi
movzbl %al,%ecx
movzbl aes128_x86mmx1_table2(,%ecx,8),%ecx
xorl %ecx,%esi
movzbl %ah,%ecx
movzwl aes128_x86mmx1_tablex(,%ecx,8),%ecx
xorl %ecx,%edi
shr  $16,%eax
movzbl %al,%ecx
movl aes128_x86mmx1_table0(,%ecx,8),%ecx
and  $0x00ff0000,%ecx
xorl %ecx,%ebp
movzbl %ah,%eax
movl aes128_x86mmx1_table1(,%eax,8),%eax
and  $0xff000000,%eax
xorl %eax,%ebx
movd %mm0,%eax
movl %ebx,0(%eax)
movl %ebp,4(%eax)
movl %edi,8(%eax)
movl %esi,12(%eax)
emms
movl 0(%esp),%eax
movl 4(%esp),%ebx
movl 8(%esp),%esi
movl 12(%esp),%edi
movl 16(%esp),%ebp
add %eax,%esp
ret
