.text
.p2align 5
.globl _aes128_x86mmx1_expand
.globl aes128_x86mmx1_expand
_aes128_x86mmx1_expand:
aes128_x86mmx1_expand:
mov %esp,%eax
and $31,%eax
add $32,%eax
sub %eax,%esp
movl %eax,0(%esp)
movl %ebx,4(%esp)
movl %esi,8(%esp)
movl %edi,12(%esp)
movl %ebp,16(%esp)
movl 4(%esp,%eax),%ecx
movl 8(%esp,%eax),%eax
movl 0(%eax),%edx
movl 4(%eax),%ebx
movl 8(%eax),%esi
movl 12(%eax),%eax
movl %edx,0(%ecx)
movl %ebx,4(%ecx)
movl %esi,8(%ecx)
movl %eax,12(%ecx)
movzbl %ah,%edi
movzbl aes128_x86mmx1_table2(,%edi,8),%edi
xor  $0x01,%edi
movzbl %al,%ebp
rol  $16,%eax
movl aes128_x86mmx1_table1(,%ebp,8),%ebp
and  $0xff000000,%ebp
xorl %ebp,%edi
movzbl %ah,%ebp
movl aes128_x86mmx1_table0(,%ebp,8),%ebp
and  $0x00ff0000,%ebp
xorl %ebp,%edi
movzbl %al,%ebp
rol  $16,%eax
movzwl aes128_x86mmx1_tablex(,%ebp,8),%ebp
xorl %ebp,%edi
xorl %edi,%edx
movl %edx,16(%ecx)
xorl %edx,%ebx
xorl %ebx,%esi
xorl %esi,%eax
movzbl %ah,%edi
movzbl aes128_x86mmx1_table2(,%edi,8),%edi
xor  $0x02,%edi
movzbl %al,%ebp
rol  $16,%eax
movl aes128_x86mmx1_table1(,%ebp,8),%ebp
and  $0xff000000,%ebp
xorl %ebp,%edi
movzbl %ah,%ebp
movl aes128_x86mmx1_table0(,%ebp,8),%ebp
and  $0x00ff0000,%ebp
xorl %ebp,%edi
movzbl %al,%ebp
rol  $16,%eax
movzwl aes128_x86mmx1_tablex(,%ebp,8),%ebp
xorl %ebp,%edi
xorl %edi,%edx
movl %edx,20(%ecx)
xorl %edx,%ebx
xorl %ebx,%esi
xorl %esi,%eax
movzbl %ah,%edi
movzbl aes128_x86mmx1_table2(,%edi,8),%edi
xor  $0x04,%edi
movzbl %al,%ebp
rol  $16,%eax
movl aes128_x86mmx1_table1(,%ebp,8),%ebp
and  $0xff000000,%ebp
xorl %ebp,%edi
movzbl %ah,%ebp
movl aes128_x86mmx1_table0(,%ebp,8),%ebp
and  $0x00ff0000,%ebp
xorl %ebp,%edi
movzbl %al,%ebp
rol  $16,%eax
movzwl aes128_x86mmx1_tablex(,%ebp,8),%ebp
xorl %ebp,%edi
xorl %edi,%edx
movl %edx,24(%ecx)
xorl %edx,%ebx
xorl %ebx,%esi
xorl %esi,%eax
movzbl %ah,%edi
movzbl aes128_x86mmx1_table2(,%edi,8),%edi
xor  $0x08,%edi
movzbl %al,%ebp
rol  $16,%eax
movl aes128_x86mmx1_table1(,%ebp,8),%ebp
and  $0xff000000,%ebp
xorl %ebp,%edi
movzbl %ah,%ebp
movl aes128_x86mmx1_table0(,%ebp,8),%ebp
and  $0x00ff0000,%ebp
xorl %ebp,%edi
movzbl %al,%ebp
rol  $16,%eax
movzwl aes128_x86mmx1_tablex(,%ebp,8),%ebp
xorl %ebp,%edi
xorl %edi,%edx
movl %edx,28(%ecx)
xorl %edx,%ebx
xorl %ebx,%esi
xorl %esi,%eax
movzbl %ah,%edi
movzbl aes128_x86mmx1_table2(,%edi,8),%edi
xor  $0x10,%edi
movzbl %al,%ebp
rol  $16,%eax
movl aes128_x86mmx1_table1(,%ebp,8),%ebp
and  $0xff000000,%ebp
xorl %ebp,%edi
movzbl %ah,%ebp
movl aes128_x86mmx1_table0(,%ebp,8),%ebp
and  $0x00ff0000,%ebp
xorl %ebp,%edi
movzbl %al,%ebp
rol  $16,%eax
movzwl aes128_x86mmx1_tablex(,%ebp,8),%ebp
xorl %ebp,%edi
xorl %edi,%edx
movl %edx,32(%ecx)
xorl %edx,%ebx
xorl %ebx,%esi
xorl %esi,%eax
movzbl %ah,%edi
movzbl aes128_x86mmx1_table2(,%edi,8),%edi
xor  $0x20,%edi
movzbl %al,%ebp
rol  $16,%eax
movl aes128_x86mmx1_table1(,%ebp,8),%ebp
and  $0xff000000,%ebp
xorl %ebp,%edi
movzbl %ah,%ebp
movl aes128_x86mmx1_table0(,%ebp,8),%ebp
and  $0x00ff0000,%ebp
xorl %ebp,%edi
movzbl %al,%ebp
rol  $16,%eax
movzwl aes128_x86mmx1_tablex(,%ebp,8),%ebp
xorl %ebp,%edi
xorl %edi,%edx
movl %edx,36(%ecx)
xorl %edx,%ebx
xorl %ebx,%esi
xorl %esi,%eax
movzbl %ah,%edi
movzbl aes128_x86mmx1_table2(,%edi,8),%edi
xor  $0x40,%edi
movzbl %al,%ebp
rol  $16,%eax
movl aes128_x86mmx1_table1(,%ebp,8),%ebp
and  $0xff000000,%ebp
xorl %ebp,%edi
movzbl %ah,%ebp
movl aes128_x86mmx1_table0(,%ebp,8),%ebp
and  $0x00ff0000,%ebp
xorl %ebp,%edi
movzbl %al,%ebp
rol  $16,%eax
movzwl aes128_x86mmx1_tablex(,%ebp,8),%ebp
xorl %ebp,%edi
xorl %edi,%edx
movl %edx,40(%ecx)
xorl %edx,%ebx
xorl %ebx,%esi
xorl %esi,%eax
movzbl %ah,%edi
movzbl aes128_x86mmx1_table2(,%edi,8),%edi
xor  $0x80,%edi
movzbl %al,%ebp
rol  $16,%eax
movl aes128_x86mmx1_table1(,%ebp,8),%ebp
and  $0xff000000,%ebp
xorl %ebp,%edi
movzbl %ah,%ebp
movl aes128_x86mmx1_table0(,%ebp,8),%ebp
and  $0x00ff0000,%ebp
xorl %ebp,%edi
movzbl %al,%ebp
rol  $16,%eax
movzwl aes128_x86mmx1_tablex(,%ebp,8),%ebp
xorl %ebp,%edi
xorl %edi,%edx
movl %edx,44(%ecx)
xorl %edx,%ebx
xorl %ebx,%esi
xorl %esi,%eax
movzbl %ah,%edi
movzbl aes128_x86mmx1_table2(,%edi,8),%edi
xor  $0x1b,%edi
movzbl %al,%ebp
rol  $16,%eax
movl aes128_x86mmx1_table1(,%ebp,8),%ebp
and  $0xff000000,%ebp
xorl %ebp,%edi
movzbl %ah,%ebp
movl aes128_x86mmx1_table0(,%ebp,8),%ebp
and  $0x00ff0000,%ebp
xorl %ebp,%edi
movzbl %al,%ebp
rol  $16,%eax
movzwl aes128_x86mmx1_tablex(,%ebp,8),%ebp
xorl %ebp,%edi
xorl %edi,%edx
movl %edx,48(%ecx)
xorl %edx,%ebx
xorl %ebx,%esi
xorl %esi,%eax
movzbl %ah,%ebx
movzbl aes128_x86mmx1_table2(,%ebx,8),%ebx
xor  $0x36,%ebx
movzbl %al,%esi
rol  $16,%eax
movl aes128_x86mmx1_table1(,%esi,8),%esi
and  $0xff000000,%esi
xorl %esi,%ebx
movzbl %ah,%esi
movl aes128_x86mmx1_table0(,%esi,8),%esi
and  $0x00ff0000,%esi
xorl %esi,%ebx
movzbl %al,%esi
rol  $16,%eax
movzwl aes128_x86mmx1_tablex(,%esi,8),%eax
xorl %eax,%ebx
xorl %ebx,%edx
movl %edx,52(%ecx)
movl 0(%esp),%eax
movl 4(%esp),%ebx
movl 8(%esp),%esi
movl 12(%esp),%edi
movl 16(%esp),%ebp
add %eax,%esp
ret
