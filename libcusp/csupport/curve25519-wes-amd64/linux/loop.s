.text
.p2align 5
.globl _curve25519_wes_loop
.globl curve25519_wes_loop
_curve25519_wes_loop:
curve25519_wes_loop:
mov %rsp,%r11
and $31,%r11
add $256,%r11
sub %r11,%rsp
movdqa %xmm6,0(%rsp)
movdqa %xmm7,16(%rsp)
movdqa %xmm8,32(%rsp)
movdqa %xmm9,48(%rsp)
movdqa %xmm10,64(%rsp)
movdqa %xmm11,80(%rsp)
movdqa %xmm12,96(%rsp)
movdqa %xmm13,112(%rsp)
movdqa %xmm14,128(%rsp)
movdqa %xmm15,144(%rsp)
movq %r11,160(%rsp)
movq %r12,168(%rsp)
movq %r13,176(%rsp)
movq %r14,184(%rsp)
movq %r15,192(%rsp)
movq %rbx,200(%rsp)
movq %rbp,208(%rsp)
movq %rdi,216(%rsp)
movq 0(%rdi),%mm0
movq 8(%rdi),%mm1
movq 16(%rdi),%mm2
movq 24(%rdi),%mm3
movq 32(%rdi),%mm4
mov  $1,%rdi
pxor   %mm5,%mm5
movd   %rdi,%xmm15
movq2dq %mm0,%xmm0
movq2dq %mm1,%xmm1
movq2dq %mm2,%xmm2
movq2dq %mm3,%xmm3
movq2dq %mm4,%xmm4
movdqa %xmm15,%xmm5
pxor   %xmm6,%xmm6
pxor   %xmm7,%xmm7
pxor   %xmm8,%xmm8
pxor   %xmm9,%xmm9
punpcklqdq %xmm0,%xmm5
punpcklqdq %xmm1,%xmm6
punpcklqdq %xmm2,%xmm7
punpcklqdq %xmm3,%xmm8
punpcklqdq %xmm4,%xmm9
movdqa %xmm15,%xmm10
pxor   %xmm11,%xmm11
pxor   %xmm12,%xmm12
pxor   %xmm13,%xmm13
pxor   %xmm14,%xmm14
pshufd $0x4e,%xmm10,%xmm10
leaq 224(%rsp),%rdi
mov  %rsi,%rsi
mov  %rdx,%rcx
rep movsb
mov  $0,%rax
mov  $32,%rcx
sub  %rdx,%rcx
rep stosb
pxor   %mm6,%mm6
sub  $1,%rdx
mov  $0xf8,%rcx
and  %rdx,%rcx
add  $8,%rcx
shl  $32,%rcx
._quadloop:
shr  $32,%rcx
je ._done
sub  $8,%rcx
leaq 224(%rsp),%rdi
movq (%rdi,%rcx),%mm5
shl  $32,%rcx
add  $64,%rcx
._bitloop:
cmp  $0,%ecx
je ._quadloop
sub  $1,%rcx
movq %mm5,%mm7
psllq $1,%mm5
psrad $31,%mm7
punpckhdq %mm7,%mm7
pxor  %mm7,%mm6
movq2dq %mm6,%xmm15
punpcklqdq %xmm15,%xmm15
movq %mm7,%mm6
paddq %xmm5,%xmm10
paddq %xmm6,%xmm11
paddq %xmm7,%xmm12
paddq %xmm8,%xmm13
paddq %xmm9,%xmm14
psllq $1,%xmm5
psllq $1,%xmm6
psllq $1,%xmm7
psllq $1,%xmm8
psllq $1,%xmm9
psubq %xmm10,%xmm5
psubq %xmm11,%xmm6
psubq %xmm12,%xmm7
psubq %xmm13,%xmm8
psubq %xmm14,%xmm9
movdqa %xmm10,%xmm0
movdqa %xmm11,%xmm1
movdqa %xmm12,%xmm2
movdqa %xmm13,%xmm3
movdqa %xmm14,%xmm4
punpckhqdq %xmm5,%xmm0
punpckhqdq %xmm6,%xmm1
punpckhqdq %xmm7,%xmm2
punpckhqdq %xmm8,%xmm3
punpckhqdq %xmm9,%xmm4
punpcklqdq %xmm5,%xmm10
punpcklqdq %xmm6,%xmm11
punpcklqdq %xmm7,%xmm12
punpcklqdq %xmm8,%xmm13
punpcklqdq %xmm9,%xmm14
movdqa %xmm0,%xmm5
movdqa %xmm1,%xmm6
movdqa %xmm2,%xmm7
movdqa %xmm3,%xmm8
movdqa %xmm4,%xmm9
psubq %xmm10,%xmm5
psubq %xmm11,%xmm6
psubq %xmm12,%xmm7
psubq %xmm13,%xmm8
psubq %xmm14,%xmm9
pand  %xmm15,%xmm5
pand  %xmm15,%xmm6
pand  %xmm15,%xmm7
pand  %xmm15,%xmm8
pand  %xmm15,%xmm9
psubq %xmm5,%xmm0
psubq %xmm6,%xmm1
psubq %xmm7,%xmm2
psubq %xmm8,%xmm3
psubq %xmm9,%xmm4
paddq %xmm5,%xmm10
paddq %xmm6,%xmm11
paddq %xmm7,%xmm12
paddq %xmm8,%xmm13
paddq %xmm9,%xmm14
pshufd $0x4e,%xmm10,%xmm5
pshufd $0x4e,%xmm11,%xmm6
pshufd $0x4e,%xmm12,%xmm7
pshufd $0x4e,%xmm13,%xmm8
pshufd $0x4e,%xmm14,%xmm9
movd   %xmm5,%r9
movd   %xmm6,%rdi
movd   %xmm7,%r10
movd   %xmm8,%r11
movd   %xmm9,%r12
call mulACtoB
movd   %r13,%xmm5
movd   %r14,%xmm6
movd   %r15,%xmm7
movd   %rbx,%xmm8
movd   %rbp,%xmm9
movd   %xmm10,%r9
movd   %xmm11,%rdi
movd   %xmm12,%r10
movd   %xmm13,%r11
movd   %xmm14,%r12
pshufd $0x4e,%xmm0,%xmm0
pshufd $0x4e,%xmm1,%xmm1
pshufd $0x4e,%xmm2,%xmm2
pshufd $0x4e,%xmm3,%xmm3
pshufd $0x4e,%xmm4,%xmm4
call mulACtoB
movd   %xmm5,%rdi
movd   %xmm6,%rsi
movd   %xmm7,%rdx
movd   %xmm8,%r8
movd   %xmm9,%r9
add  %rdi,%r13
add  %rsi,%r14
add  %rdx,%r15
add  %r8,%rbx
add  %r9,%rbp
shl  $1,%rdi
shl  $1,%rsi
shl  $1,%rdx
shl  $1,%r8
shl  $1,%r9
sub  %r13,%rdi
sub  %r14,%rsi
sub  %r15,%rdx
sub  %rbx,%r8
sub  %rbp,%r9
movd   %rdi,%xmm0
movd   %rsi,%xmm1
movd   %rdx,%xmm2
movd   %r8,%xmm3
movd   %r9,%xmm4
call sqrBtoA
movd   %r9,%xmm5
movd   %rdi,%xmm6
movd   %r10,%xmm7
movd   %r11,%xmm8
movd   %r12,%xmm9
movd   %xmm0,%r13
movd   %xmm1,%r14
movd   %xmm2,%r15
movd   %xmm3,%rbx
movd   %xmm4,%rbp
movq2dq %mm0,%xmm0
movq2dq %mm1,%xmm1
movq2dq %mm2,%xmm2
movq2dq %mm3,%xmm3
movq2dq %mm4,%xmm4
call sqrBtoA
call mulACtoB
movd   %r13,%xmm0
movd   %r14,%xmm1
movd   %r15,%xmm2
movd   %rbx,%xmm3
movd   %rbp,%xmm4
punpcklqdq %xmm0,%xmm5
punpcklqdq %xmm1,%xmm6
punpcklqdq %xmm2,%xmm7
punpcklqdq %xmm3,%xmm8
punpcklqdq %xmm4,%xmm9
movd   %xmm10,%r13
movd   %xmm11,%r14
movd   %xmm12,%r15
movd   %xmm13,%rbx
movd   %xmm14,%rbp
call sqrBtoA
movd   %r9,%xmm0
movd   %rdi,%xmm1
movd   %r10,%xmm2
movd   %r11,%xmm3
movd   %r12,%xmm4
pshufd $0x4e,%xmm10,%xmm10
pshufd $0x4e,%xmm11,%xmm11
pshufd $0x4e,%xmm12,%xmm12
pshufd $0x4e,%xmm13,%xmm13
pshufd $0x4e,%xmm14,%xmm14
movd   %xmm10,%r13
movd   %xmm11,%r14
movd   %xmm12,%r15
movd   %xmm13,%rbx
movd   %xmm14,%rbp
call sqrBtoA
call mulACtoB
movd   %r13,%xmm10
movd   %r14,%xmm11
movd   %r15,%xmm12
movd   %rbx,%xmm13
movd   %rbp,%xmm14
movd   %xmm0,%r13
movd   %xmm1,%r14
movd   %xmm2,%r15
movd   %xmm3,%rbx
movd   %xmm4,%rbp
sub  %r9,%r13
sub  %rdi,%r14
sub  %r10,%r15
sub  %r11,%rbx
sub  %r12,%rbp
call scalBtoA
movd   %xmm0,%rsi
movd   %xmm1,%rdx
movd   %xmm2,%r8
add  %rsi,%r9
add  %rdx,%rdi
add  %r8,%r10
movd   %r13,%xmm0
movd   %r14,%xmm1
movd   %r15,%xmm2
movd   %xmm3,%rsi
movd   %xmm4,%rdx
add  %rsi,%r11
add  %rdx,%r12
movd   %rbx,%xmm3
movd   %rbp,%xmm4
call mulACtoB
movd   %r13,%xmm0
movd   %r14,%xmm1
movd   %r15,%xmm2
movd   %rbx,%xmm3
movd   %rbp,%xmm4
punpcklqdq %xmm5,%xmm10
punpcklqdq %xmm6,%xmm11
punpcklqdq %xmm7,%xmm12
punpcklqdq %xmm8,%xmm13
punpcklqdq %xmm9,%xmm14
pshufd $0x4e,%xmm5,%xmm5
pshufd $0x4e,%xmm6,%xmm6
pshufd $0x4e,%xmm7,%xmm7
pshufd $0x4e,%xmm8,%xmm8
pshufd $0x4e,%xmm9,%xmm9
punpcklqdq %xmm5,%xmm0
punpcklqdq %xmm6,%xmm1
punpcklqdq %xmm7,%xmm2
punpcklqdq %xmm8,%xmm3
punpcklqdq %xmm9,%xmm4
movdqa %xmm10,%xmm5
movdqa %xmm11,%xmm6
movdqa %xmm12,%xmm7
movdqa %xmm13,%xmm8
movdqa %xmm14,%xmm9
movdqa %xmm0,%xmm10
movdqa %xmm1,%xmm11
movdqa %xmm2,%xmm12
movdqa %xmm3,%xmm13
movdqa %xmm4,%xmm14
jmp ._bitloop
._done:
movdqa %xmm10,%xmm0
movdqa %xmm11,%xmm1
movdqa %xmm12,%xmm2
movdqa %xmm13,%xmm3
movdqa %xmm14,%xmm4
punpckhqdq %xmm5,%xmm0
punpckhqdq %xmm6,%xmm1
punpckhqdq %xmm7,%xmm2
punpckhqdq %xmm8,%xmm3
punpckhqdq %xmm9,%xmm4
punpcklqdq %xmm5,%xmm10
punpcklqdq %xmm6,%xmm11
punpcklqdq %xmm7,%xmm12
punpcklqdq %xmm8,%xmm13
punpcklqdq %xmm9,%xmm14
movq2dq %mm6,%xmm15
punpcklqdq %xmm15,%xmm15
movdqa %xmm0,%xmm5
movdqa %xmm1,%xmm6
movdqa %xmm2,%xmm7
movdqa %xmm3,%xmm8
movdqa %xmm4,%xmm9
psubq %xmm10,%xmm5
psubq %xmm11,%xmm6
psubq %xmm12,%xmm7
psubq %xmm13,%xmm8
psubq %xmm14,%xmm9
pand  %xmm15,%xmm5
pand  %xmm15,%xmm6
pand  %xmm15,%xmm7
pand  %xmm15,%xmm8
pand  %xmm15,%xmm9
psubq %xmm5,%xmm0
psubq %xmm6,%xmm1
psubq %xmm7,%xmm2
psubq %xmm8,%xmm3
psubq %xmm9,%xmm4
paddq %xmm5,%xmm10
paddq %xmm6,%xmm11
paddq %xmm7,%xmm12
paddq %xmm8,%xmm13
paddq %xmm9,%xmm14
movq 216(%rsp),%rdi
movd   %xmm10,%rsi
movd   %xmm11,%rdx
movd   %xmm12,%rcx
movd   %xmm13,%r8
movd   %xmm14,%r9
pshufd $0x4e,%xmm10,%xmm10
pshufd $0x4e,%xmm11,%xmm11
pshufd $0x4e,%xmm12,%xmm12
pshufd $0x4e,%xmm13,%xmm13
pshufd $0x4e,%xmm14,%xmm14
movd   %xmm10,%rax
movd   %xmm11,%r10
movd   %xmm12,%r11
movd   %xmm13,%r12
movd   %xmm14,%r13
movq   %rax,0(%rdi)
movq   %r10,8(%rdi)
movq   %r11,16(%rdi)
movq   %r12,24(%rdi)
movq   %r13,32(%rdi)
movq   %rsi,40(%rdi)
movq   %rdx,48(%rdi)
movq   %rcx,56(%rdi)
movq   %r8,64(%rdi)
movq   %r9,72(%rdi)
movd   %xmm0,%rsi
movd   %xmm1,%rdx
movd   %xmm2,%rcx
movd   %xmm3,%r8
movd   %xmm4,%r9
pshufd $0x4e,%xmm0,%xmm0
pshufd $0x4e,%xmm1,%xmm1
pshufd $0x4e,%xmm2,%xmm2
pshufd $0x4e,%xmm3,%xmm3
pshufd $0x4e,%xmm4,%xmm4
movd   %xmm0,%rax
movd   %xmm1,%r10
movd   %xmm2,%r11
movd   %xmm3,%r12
movd   %xmm4,%r13
movq   %rax,80(%rdi)
movq   %r10,88(%rdi)
movq   %r11,96(%rdi)
movq   %r12,104(%rdi)
movq   %r13,112(%rdi)
movq   %rsi,120(%rdi)
movq   %rdx,128(%rdi)
movq   %rcx,136(%rdi)
movq   %r8,144(%rdi)
movq   %r9,152(%rdi)
movdqa 0(%rsp),%xmm6
movdqa 16(%rsp),%xmm7
movdqa 32(%rsp),%xmm8
movdqa 48(%rsp),%xmm9
movdqa 64(%rsp),%xmm10
movdqa 80(%rsp),%xmm11
movdqa 96(%rsp),%xmm12
movdqa 112(%rsp),%xmm13
movdqa 128(%rsp),%xmm14
movdqa 144(%rsp),%xmm15
movq 160(%rsp),%r11
movq 168(%rsp),%r12
movq 176(%rsp),%r13
movq 184(%rsp),%r14
movq 192(%rsp),%r15
movq 200(%rsp),%rbx
movq 208(%rsp),%rbp
emms
add %r11,%rsp
ret
