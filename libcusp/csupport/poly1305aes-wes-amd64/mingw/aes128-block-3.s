.text
.p2align 5
.globl _aes128_amd64_2_block
.globl aes128_amd64_2_block
_aes128_amd64_2_block:
aes128_amd64_2_block:
mov %rsp,%r11
lea aes128_amd64_2_constants(%rip),%r10
sub %r10,%r11
and $4095,%r11
add $96,%r11
sub %r11,%rsp
movq %rcx,0(%rsp)
movq %r11,8(%rsp)
movq %r12,16(%rsp)
movq %r13,24(%rsp)
movq %r14,32(%rsp)
movq %r15,40(%rsp)
movq %rdi,48(%rsp)
movq %rsi,56(%rsp)
movq %rbp,64(%rsp)
movq %rbx,72(%rsp)
movl   0(%r8),%r9d
movl   4(%r8),%r10d
movl   8(%r8),%r11d
movl   12(%r8),%r12d
movl   0(%rdx),%ecx
movl   4(%rdx),%ebx
movl   8(%rdx),%eax
movl   12(%rdx),%edx
lea  aes128_amd64_2_tablex(%rip),%r13
xor  %r9,%rcx
xor  %r10,%rbx
xor  %r11,%rax
xor  %r12,%rdx
movl   16(%r8),%r9d
xor  %r9,%r10
xor  %r10,%r11
xor  %r11,%r12
movzbl  %cl,%r14d
movzbl  %ch,%edi
shr  $16,%ecx
movzbl  %cl,%r15d
movzbl  %ch,%esi
movl   3(%r13,%r14,8),%ecx
xor  %r9,%rcx
movzbl  %bl,%r9d
movzbl  %bh,%ebp
xorl 2(%r13,%rbp,8),%ecx
shr  $16,%ebx
movzbl  %bl,%r14d
movzbl  %bh,%ebp
movl   4(%r13,%rsi,8),%ebx
xorl 3(%r13,%r9,8),%ebx
movzbl  %al,%r9d
movzbl  %ah,%esi
xorl 2(%r13,%rsi,8),%ebx
shr  $16,%eax
movzbl  %al,%esi
xorl 1(%r13,%rsi,8),%ecx
movzbl  %ah,%esi
movl   1(%r13,%r15,8),%eax
xorl 4(%r13,%rbp,8),%eax
xorl 3(%r13,%r9,8),%eax
movzbl  %dl,%r9d
movzbl  %dh,%ebp
xorl 2(%r13,%rbp,8),%eax
shr  $16,%edx
movzbl  %dl,%r15d
xorl 1(%r13,%r15,8),%ebx
movzbl  %dh,%ebp
xorl 4(%r13,%rbp,8),%ecx
movl   2(%r13,%rdi,8),%edx
xor  %r11,%rax
xorl 1(%r13,%r14,8),%edx
xor  %r10,%rbx
xorl 4(%r13,%rsi,8),%edx
xorl 3(%r13,%r9,8),%edx
xor  %r12,%rdx
movl   20(%r8),%r9d
xor  %r9,%r10
xor  %r10,%r11
xor  %r11,%r12
movzbl  %cl,%r14d
movzbl  %ch,%edi
shr  $16,%ecx
movzbl  %cl,%r15d
movzbl  %ch,%esi
movl   3(%r13,%r14,8),%ecx
xor  %r9,%rcx
movzbl  %bl,%r9d
movzbl  %bh,%ebp
xorl 2(%r13,%rbp,8),%ecx
shr  $16,%ebx
movzbl  %bl,%r14d
movzbl  %bh,%ebp
movl   4(%r13,%rsi,8),%ebx
xorl 3(%r13,%r9,8),%ebx
movzbl  %al,%r9d
movzbl  %ah,%esi
xorl 2(%r13,%rsi,8),%ebx
shr  $16,%eax
movzbl  %al,%esi
xorl 1(%r13,%rsi,8),%ecx
movzbl  %ah,%esi
movl   1(%r13,%r15,8),%eax
xorl 4(%r13,%rbp,8),%eax
xorl 3(%r13,%r9,8),%eax
movzbl  %dl,%r9d
movzbl  %dh,%ebp
xorl 2(%r13,%rbp,8),%eax
shr  $16,%edx
movzbl  %dl,%r15d
xorl 1(%r13,%r15,8),%ebx
movzbl  %dh,%ebp
xorl 4(%r13,%rbp,8),%ecx
movl   2(%r13,%rdi,8),%edx
xor  %r11,%rax
xorl 1(%r13,%r14,8),%edx
xor  %r10,%rbx
xorl 4(%r13,%rsi,8),%edx
xorl 3(%r13,%r9,8),%edx
xor  %r12,%rdx
movl   24(%r8),%r9d
xor  %r9,%r10
xor  %r10,%r11
xor  %r11,%r12
movzbl  %cl,%r14d
movzbl  %ch,%edi
shr  $16,%ecx
movzbl  %cl,%r15d
movzbl  %ch,%esi
movl   3(%r13,%r14,8),%ecx
xor  %r9,%rcx
movzbl  %bl,%r9d
movzbl  %bh,%ebp
xorl 2(%r13,%rbp,8),%ecx
shr  $16,%ebx
movzbl  %bl,%r14d
movzbl  %bh,%ebp
movl   4(%r13,%rsi,8),%ebx
xorl 3(%r13,%r9,8),%ebx
movzbl  %al,%r9d
movzbl  %ah,%esi
xorl 2(%r13,%rsi,8),%ebx
shr  $16,%eax
movzbl  %al,%esi
xorl 1(%r13,%rsi,8),%ecx
movzbl  %ah,%esi
movl   1(%r13,%r15,8),%eax
xorl 4(%r13,%rbp,8),%eax
xorl 3(%r13,%r9,8),%eax
movzbl  %dl,%r9d
movzbl  %dh,%ebp
xorl 2(%r13,%rbp,8),%eax
shr  $16,%edx
movzbl  %dl,%r15d
xorl 1(%r13,%r15,8),%ebx
movzbl  %dh,%ebp
xorl 4(%r13,%rbp,8),%ecx
movl   2(%r13,%rdi,8),%edx
xorl 1(%r13,%r14,8),%edx
xorl 4(%r13,%rsi,8),%edx
xor  %r10,%rbx
xorl 3(%r13,%r9,8),%edx
xor  %r11,%rax
xor  %r12,%rdx
movl   28(%r8),%r9d
xor  %r9,%r10
xor  %r10,%r11
xor  %r11,%r12
movzbl  %cl,%r14d
movzbl  %ch,%edi
shr  $16,%ecx
movzbl  %cl,%r15d
movzbl  %ch,%esi
movl   3(%r13,%r14,8),%ecx
xor  %r9,%rcx
movzbl  %bl,%r9d
movzbl  %bh,%ebp
xorl 2(%r13,%rbp,8),%ecx
shr  $16,%ebx
movzbl  %bl,%r14d
movzbl  %bh,%ebp
movl   4(%r13,%rsi,8),%ebx
xorl 3(%r13,%r9,8),%ebx
movzbl  %al,%r9d
movzbl  %ah,%esi
xorl 2(%r13,%rsi,8),%ebx
shr  $16,%eax
movzbl  %al,%esi
xorl 1(%r13,%rsi,8),%ecx
movzbl  %ah,%esi
movl   1(%r13,%r15,8),%eax
xorl 4(%r13,%rbp,8),%eax
xorl 3(%r13,%r9,8),%eax
movzbl  %dl,%r9d
movzbl  %dh,%ebp
xorl 2(%r13,%rbp,8),%eax
shr  $16,%edx
movzbl  %dl,%r15d
xorl 1(%r13,%r15,8),%ebx
movzbl  %dh,%ebp
xorl 4(%r13,%rbp,8),%ecx
movl   2(%r13,%rdi,8),%edx
xorl 1(%r13,%r14,8),%edx
xorl 4(%r13,%rsi,8),%edx
xor  %r10,%rbx
xorl 3(%r13,%r9,8),%edx
xor  %r11,%rax
xor  %r12,%rdx
movl   32(%r8),%r9d
xor  %r9,%r10
xor  %r10,%r11
xor  %r11,%r12
movzbl  %cl,%r14d
movzbl  %ch,%edi
shr  $16,%ecx
movzbl  %cl,%r15d
movzbl  %ch,%esi
movl   3(%r13,%r14,8),%ecx
xor  %r9,%rcx
movzbl  %bl,%r9d
movzbl  %bh,%ebp
xorl 2(%r13,%rbp,8),%ecx
shr  $16,%ebx
movzbl  %bl,%r14d
movzbl  %bh,%ebp
movl   4(%r13,%rsi,8),%ebx
xorl 3(%r13,%r9,8),%ebx
movzbl  %al,%r9d
movzbl  %ah,%esi
xorl 2(%r13,%rsi,8),%ebx
shr  $16,%eax
movzbl  %al,%esi
xorl 1(%r13,%rsi,8),%ecx
movzbl  %ah,%esi
movl   1(%r13,%r15,8),%eax
xorl 4(%r13,%rbp,8),%eax
xorl 3(%r13,%r9,8),%eax
movzbl  %dl,%r9d
movzbl  %dh,%ebp
xorl 2(%r13,%rbp,8),%eax
shr  $16,%edx
movzbl  %dl,%r15d
xorl 1(%r13,%r15,8),%ebx
movzbl  %dh,%ebp
xorl 4(%r13,%rbp,8),%ecx
movl   2(%r13,%rdi,8),%edx
xorl 1(%r13,%r14,8),%edx
xorl 4(%r13,%rsi,8),%edx
xor  %r10,%rbx
xorl 3(%r13,%r9,8),%edx
xor  %r11,%rax
xor  %r12,%rdx
movl   36(%r8),%r9d
xor  %r9,%r10
xor  %r10,%r11
xor  %r11,%r12
movzbl  %cl,%r14d
movzbl  %ch,%edi
shr  $16,%ecx
movzbl  %cl,%r15d
movzbl  %ch,%esi
movl   3(%r13,%r14,8),%ecx
xor  %r9,%rcx
movzbl  %bl,%r9d
movzbl  %bh,%ebp
xorl 2(%r13,%rbp,8),%ecx
shr  $16,%ebx
movzbl  %bl,%r14d
movzbl  %bh,%ebp
movl   4(%r13,%rsi,8),%ebx
xorl 3(%r13,%r9,8),%ebx
movzbl  %al,%r9d
movzbl  %ah,%esi
xorl 2(%r13,%rsi,8),%ebx
shr  $16,%eax
movzbl  %al,%esi
xorl 1(%r13,%rsi,8),%ecx
movzbl  %ah,%esi
movl   1(%r13,%r15,8),%eax
xorl 4(%r13,%rbp,8),%eax
xorl 3(%r13,%r9,8),%eax
movzbl  %dl,%r9d
movzbl  %dh,%ebp
xorl 2(%r13,%rbp,8),%eax
shr  $16,%edx
movzbl  %dl,%r15d
xorl 1(%r13,%r15,8),%ebx
movzbl  %dh,%ebp
xorl 4(%r13,%rbp,8),%ecx
movl   2(%r13,%rdi,8),%edx
xorl 1(%r13,%r14,8),%edx
xorl 4(%r13,%rsi,8),%edx
xor  %r10,%rbx
xorl 3(%r13,%r9,8),%edx
xor  %r11,%rax
xor  %r12,%rdx
movl   40(%r8),%r9d
xor  %r9,%r10
xor  %r10,%r11
xor  %r11,%r12
movzbl  %cl,%r14d
movzbl  %ch,%edi
shr  $16,%ecx
movzbl  %cl,%r15d
movzbl  %ch,%esi
movl   3(%r13,%r14,8),%ecx
xor  %r9,%rcx
movzbl  %bl,%r9d
movzbl  %bh,%ebp
xorl 2(%r13,%rbp,8),%ecx
shr  $16,%ebx
movzbl  %bl,%r14d
movzbl  %bh,%ebp
movl   4(%r13,%rsi,8),%ebx
xorl 3(%r13,%r9,8),%ebx
movzbl  %al,%r9d
movzbl  %ah,%esi
xorl 2(%r13,%rsi,8),%ebx
shr  $16,%eax
movzbl  %al,%esi
xorl 1(%r13,%rsi,8),%ecx
movzbl  %ah,%esi
movl   1(%r13,%r15,8),%eax
xorl 4(%r13,%rbp,8),%eax
xorl 3(%r13,%r9,8),%eax
movzbl  %dl,%r9d
movzbl  %dh,%ebp
xorl 2(%r13,%rbp,8),%eax
shr  $16,%edx
movzbl  %dl,%r15d
xorl 1(%r13,%r15,8),%ebx
movzbl  %dh,%ebp
xorl 4(%r13,%rbp,8),%ecx
movl   2(%r13,%rdi,8),%edx
xorl 1(%r13,%r14,8),%edx
xorl 4(%r13,%rsi,8),%edx
xor  %r10,%rbx
xorl 3(%r13,%r9,8),%edx
xor  %r11,%rax
xor  %r12,%rdx
movl   44(%r8),%r9d
xor  %r9,%r10
xor  %r10,%r11
xor  %r11,%r12
movzbl  %cl,%r14d
movzbl  %ch,%edi
shr  $16,%ecx
movzbl  %cl,%r15d
movzbl  %ch,%esi
movl   3(%r13,%r14,8),%ecx
xor  %r9,%rcx
movzbl  %bl,%r9d
movzbl  %bh,%ebp
xorl 2(%r13,%rbp,8),%ecx
shr  $16,%ebx
movzbl  %bl,%r14d
movzbl  %bh,%ebp
movl   4(%r13,%rsi,8),%ebx
xorl 3(%r13,%r9,8),%ebx
movzbl  %al,%r9d
movzbl  %ah,%esi
xorl 2(%r13,%rsi,8),%ebx
shr  $16,%eax
movzbl  %al,%esi
xorl 1(%r13,%rsi,8),%ecx
movzbl  %ah,%esi
movl   1(%r13,%r15,8),%eax
xorl 4(%r13,%rbp,8),%eax
xorl 3(%r13,%r9,8),%eax
movzbl  %dl,%r9d
movzbl  %dh,%ebp
xorl 2(%r13,%rbp,8),%eax
shr  $16,%edx
movzbl  %dl,%r15d
xorl 1(%r13,%r15,8),%ebx
movzbl  %dh,%ebp
xorl 4(%r13,%rbp,8),%ecx
movl   2(%r13,%rdi,8),%edx
xorl 1(%r13,%r14,8),%edx
xorl 4(%r13,%rsi,8),%edx
xor  %r10,%rbx
xorl 3(%r13,%r9,8),%edx
xor  %r11,%rax
xor  %r12,%rdx
movl   48(%r8),%r9d
xor  %r9,%r10
xor  %r10,%r11
xor  %r11,%r12
movzbl  %cl,%r14d
movzbl  %ch,%edi
shr  $16,%ecx
movzbl  %cl,%r15d
movzbl  %ch,%esi
movl   3(%r13,%r14,8),%ecx
xor  %r9,%rcx
movzbl  %bl,%r9d
movzbl  %bh,%ebp
xorl 2(%r13,%rbp,8),%ecx
shr  $16,%ebx
movzbl  %bl,%r14d
movzbl  %bh,%ebp
movl   4(%r13,%rsi,8),%ebx
xorl 3(%r13,%r9,8),%ebx
movzbl  %al,%r9d
movzbl  %ah,%esi
xorl 2(%r13,%rsi,8),%ebx
shr  $16,%eax
movzbl  %al,%esi
xorl 1(%r13,%rsi,8),%ecx
movzbl  %ah,%esi
movl   1(%r13,%r15,8),%eax
xorl 4(%r13,%rbp,8),%eax
xorl 3(%r13,%r9,8),%eax
movzbl  %dl,%r9d
movzbl  %dh,%ebp
xorl 2(%r13,%rbp,8),%eax
shr  $16,%edx
movzbl  %dl,%r15d
xorl 1(%r13,%r15,8),%ebx
movzbl  %dh,%ebp
xorl 4(%r13,%rbp,8),%ecx
movl   2(%r13,%rdi,8),%edx
xorl 1(%r13,%r14,8),%edx
xorl 4(%r13,%rsi,8),%edx
xor  %r10,%rbx
xorl 3(%r13,%r9,8),%edx
xor  %r11,%rax
xor  %r12,%rdx
movl   52(%r8),%r8d
xor  %r8,%r10
xor  %r10,%r11
xor  %r11,%r12
movzbl  %cl,%r9d
movzbq 1(%r13,%r9,8),%r9
movzbl  %ch,%edi
movzwq (%r13,%rdi,8),%r14
shr  $16,%ecx
movzbl  %cl,%r15d
movl   3(%r13,%r15,8),%r15d
and  $0x00ff0000,%r15d
movzbl  %ch,%edi
movl   2(%r13,%rdi,8),%ecx
and  $0xff000000,%ecx
xor  %r8,%r9
xor  %r12,%r14
xor  %r10,%rcx
xor  %r11,%r15
movzbl  %bl,%r8d
movzbq 1(%r13,%r8,8),%r8
xor  %r8,%rcx
movzbl  %bh,%edi
movzwq (%r13,%rdi,8),%r8
xor  %r8,%r9
shr  $16,%ebx
movzbl  %bl,%r8d
movl   3(%r13,%r8,8),%r8d
and  $0x00ff0000,%r8d
xor  %r8,%r14
movzbl  %bh,%edi
movl   2(%r13,%rdi,8),%r8d
and  $0xff000000,%r8d
xor  %r8,%r15
movzbl  %al,%r8d
movzbq 1(%r13,%r8,8),%r8
xor  %r8,%r15
movzbl  %ah,%edi
movzwq (%r13,%rdi,8),%r8
xor  %r8,%rcx
shr  $16,%eax
movzbl  %al,%r8d
movl   3(%r13,%r8,8),%r8d
and  $0x00ff0000,%r8d
xor  %r8,%r9
movzbl  %ah,%edi
movl   2(%r13,%rdi,8),%r8d
and  $0xff000000,%r8d
xor  %r8,%r14
movzbl  %dl,%r8d
movzbq 1(%r13,%r8,8),%r8
xor  %r8,%r14
movzbl  %dh,%edi
movzwq (%r13,%rdi,8),%r8
xor  %r8,%r15
shr  $16,%edx
movzbl  %dl,%r8d
movl   3(%r13,%r8,8),%r8d
and  $0x00ff0000,%r8d
xor  %r8,%rcx
movzbl  %dh,%edi
movl   2(%r13,%rdi,8),%edx
and  $0xff000000,%edx
xor  %rdx,%r9
movq 0(%rsp),%rdx
movl   %r9d,0(%rdx)
movl   %ecx,4(%rdx)
movl   %r15d,8(%rdx)
movl   %r14d,12(%rdx)
movq 8(%rsp),%r11
movq 16(%rsp),%r12
movq 24(%rsp),%r13
movq 32(%rsp),%r14
movq 40(%rsp),%r15
movq 48(%rsp),%rdi
movq 56(%rsp),%rsi
movq 64(%rsp),%rbp
movq 72(%rsp),%rbx
add %r11,%rsp
ret
