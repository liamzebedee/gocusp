.text
.p2align 5
.globl _poly1305_ppro
.globl poly1305_ppro
_poly1305_ppro:
poly1305_ppro:
mov %esp,%eax
sub $poly1305_ppro_constants,%eax
and $4095,%eax
add $144,%eax
sub %eax,%esp
fldcw poly1305_ppro_rounding
movl %ebx,104(%esp)
movl %esi,108(%esp)
movl %edi,112(%esp)
movl %ebp,116(%esp)
movl 8(%esp,%eax),%ecx
fildl 0(%ecx)
fstpl 48(%esp)
fildl 4(%ecx)
fmull 0+poly1305_ppro_two32
fstl 56(%esp)
fmull 0+poly1305_ppro_scale
fstpl 80(%esp)
fildl 8(%ecx)
fmull 0+poly1305_ppro_two64
fstl 64(%esp)
fmull 0+poly1305_ppro_scale
fstpl 88(%esp)
fildl 12(%ecx)
fmull 0+poly1305_ppro_two96
fstl 72(%esp)
fmull 0+poly1305_ppro_scale
fstpl 96(%esp)
fldz
fldz
fldz
fldz
movl $0x43300000,20(%esp)
movl $0x45300000,28(%esp)
movl $0x47300000,36(%esp)
movl $0x49300000,44(%esp)
movl 16(%esp,%eax),%esi
movl 20(%esp,%eax),%ecx
movl %eax,120(%esp)
cmp $16,%ecx
jb ._addatmost15bytes
movl 12(%esi),%edi
movl 8(%esi),%ebx
movl 4(%esi),%edx
movl 0(%esi),%eax
movl %edi,40(%esp)
movl %ebx,32(%esp)
movl %edx,24(%esp)
movl %eax,16(%esp)
add $16,%esi
sub $16,%ecx
fxch %st(3)
faddl 40(%esp)
fsubl 0+poly1305_ppro_doffset3minustwo128
fxch %st(2)
faddl 32(%esp)
fsubl 0+poly1305_ppro_doffset2
fxch %st(1)
faddl 24(%esp)
fsubl 0+poly1305_ppro_doffset1
fxch %st(3)
faddl 16(%esp)
fsubl 0+poly1305_ppro_doffset0
cmp $16,%ecx
jb ._multiplyaddatmost15bytes
._multiplyaddatleast16bytes:
movl 12(%esi),%edi
movl 8(%esi),%ebx
movl 4(%esi),%edx
movl 0(%esi),%eax
movl %edi,40(%esp)
movl %ebx,32(%esp)
movl %edx,24(%esp)
movl %eax,16(%esp)
add $16,%esi
sub $16,%ecx
fldl 0+poly1305_ppro_alpha130
fadd %st(3),%st(0)
fsubl 0+poly1305_ppro_alpha130
fsubr %st(0),%st(3)
fmull 0+poly1305_ppro_scale
fldl 0+poly1305_ppro_alpha32
fadd %st(2),%st(0)
fsubl 0+poly1305_ppro_alpha32
fsubr %st(0),%st(2)
fxch %st(2)
faddp %st(0),%st(1)
fldl 0+poly1305_ppro_alpha64
fadd %st(5),%st(0)
fsubl 0+poly1305_ppro_alpha64
fsubr %st(0),%st(5)
fldl 0+poly1305_ppro_alpha96
fadd %st(4),%st(0)
fsubl 0+poly1305_ppro_alpha96
fsubr %st(0),%st(4)
fxch %st(4)
faddp %st(0),%st(1)
fxch %st(4)
faddp %st(0),%st(3)
fxch %st(4)
faddp %st(0),%st(1)
fldl 72(%esp)
fmul %st(4),%st(0)
fldl 64(%esp)
fmul %st(5),%st(0)
fldl 56(%esp)
fmul %st(6),%st(0)
fldl 48(%esp)
fmulp %st(0),%st(7)
fldl 64(%esp)
fmul %st(4),%st(0)
faddp %st(0),%st(3)
fldl 56(%esp)
fmul %st(4),%st(0)
faddp %st(0),%st(2)
fldl 48(%esp)
fmul %st(4),%st(0)
faddp %st(0),%st(1)
fldl 96(%esp)
fmulp %st(0),%st(4)
fxch %st(3)
faddp %st(0),%st(6)
fldl 56(%esp)
fmul %st(5),%st(0)
faddp %st(0),%st(2)
fldl 48(%esp)
fmul %st(5),%st(0)
faddp %st(0),%st(1)
fldl 96(%esp)
fmul %st(5),%st(0)
faddp %st(0),%st(3)
fldl 88(%esp)
fmulp %st(0),%st(5)
fxch %st(4)
faddp %st(0),%st(5)
fldl 48(%esp)
fmul %st(3),%st(0)
faddp %st(0),%st(1)
fxch %st(4)
fldl 96(%esp)
fmul %st(3),%st(0)
faddp %st(0),%st(4)
fxch %st(1)
fldl 88(%esp)
fmul %st(3),%st(0)
faddp %st(0),%st(1)
fldl 80(%esp)
fmulp %st(0),%st(3)
fxch %st(2)
faddp %st(0),%st(1)
cmp $16,%ecx
fxch %st(3)
faddl 40(%esp)
fsubl 0+poly1305_ppro_doffset3minustwo128
fxch %st(2)
faddl 32(%esp)
fsubl 0+poly1305_ppro_doffset2
fxch %st(1)
faddl 24(%esp)
fsubl 0+poly1305_ppro_doffset1
fxch %st(3)
faddl 16(%esp)
fsubl 0+poly1305_ppro_doffset0
jae ._multiplyaddatleast16bytes
._multiplyaddatmost15bytes:
fldl 0+poly1305_ppro_alpha130
fadd %st(3),%st(0)
fsubl 0+poly1305_ppro_alpha130
fsubr %st(0),%st(3)
fmull 0+poly1305_ppro_scale
fldl 0+poly1305_ppro_alpha32
fadd %st(2),%st(0)
fsubl 0+poly1305_ppro_alpha32
fsubr %st(0),%st(2)
fldl 0+poly1305_ppro_alpha64
fadd %st(6),%st(0)
fsubl 0+poly1305_ppro_alpha64
fsubr %st(0),%st(6)
fldl 0+poly1305_ppro_alpha96
fadd %st(5),%st(0)
fsubl 0+poly1305_ppro_alpha96
fsubr %st(0),%st(5)
fxch %st(4)
faddp %st(0),%st(3)
fxch %st(6)
faddp %st(0),%st(1)
fxch %st(3)
faddp %st(0),%st(5)
fxch %st(3)
faddp %st(0),%st(1)
fldl 72(%esp)
fmul %st(3),%st(0)
fldl 64(%esp)
fmul %st(4),%st(0)
fldl 56(%esp)
fmul %st(5),%st(0)
fldl 48(%esp)
fmulp %st(0),%st(6)
fldl 64(%esp)
fmul %st(5),%st(0)
faddp %st(0),%st(3)
fldl 56(%esp)
fmul %st(5),%st(0)
faddp %st(0),%st(2)
fldl 48(%esp)
fmul %st(5),%st(0)
faddp %st(0),%st(1)
fldl 96(%esp)
fmulp %st(0),%st(5)
fxch %st(4)
faddp %st(0),%st(5)
fldl 56(%esp)
fmul %st(6),%st(0)
faddp %st(0),%st(2)
fldl 48(%esp)
fmul %st(6),%st(0)
faddp %st(0),%st(1)
fldl 96(%esp)
fmul %st(6),%st(0)
faddp %st(0),%st(4)
fldl 88(%esp)
fmulp %st(0),%st(6)
fxch %st(5)
faddp %st(0),%st(4)
fldl 48(%esp)
fmul %st(2),%st(0)
faddp %st(0),%st(1)
fldl 96(%esp)
fmul %st(2),%st(0)
faddp %st(0),%st(5)
fldl 88(%esp)
fmul %st(2),%st(0)
faddp %st(0),%st(3)
fldl 80(%esp)
fmulp %st(0),%st(2)
fxch %st(1)
faddp %st(0),%st(3)
fxch %st(3)
fxch %st(2)
._addatmost15bytes:
cmp $0,%ecx
je ._nomorebytes
movl $0,0(%esp)
movl $0,4(%esp)
movl $0,8(%esp)
movl $0,12(%esp)
leal 0(%esp),%edi
rep movsb
movb $1,0(%edi)
movl 12(%esp),%ebx
movl 8(%esp),%edx
movl 4(%esp),%ecx
movl 0(%esp),%eax
movl %ebx,40(%esp)
movl %edx,32(%esp)
movl %ecx,24(%esp)
movl %eax,16(%esp)
fxch %st(3)
faddl 40(%esp)
fsubl 0+poly1305_ppro_doffset3
fxch %st(2)
faddl 32(%esp)
fsubl 0+poly1305_ppro_doffset2
fxch %st(1)
faddl 24(%esp)
fsubl 0+poly1305_ppro_doffset1
fxch %st(3)
faddl 16(%esp)
fsubl 0+poly1305_ppro_doffset0
fldl 0+poly1305_ppro_alpha130
fadd %st(3),%st(0)
fsubl 0+poly1305_ppro_alpha130
fsubr %st(0),%st(3)
fmull 0+poly1305_ppro_scale
fldl 0+poly1305_ppro_alpha32
fadd %st(2),%st(0)
fsubl 0+poly1305_ppro_alpha32
fsubr %st(0),%st(2)
fldl 0+poly1305_ppro_alpha64
fadd %st(6),%st(0)
fsubl 0+poly1305_ppro_alpha64
fsubr %st(0),%st(6)
fldl 0+poly1305_ppro_alpha96
fadd %st(5),%st(0)
fsubl 0+poly1305_ppro_alpha96
fsubr %st(0),%st(5)
fxch %st(4)
faddp %st(0),%st(3)
fxch %st(6)
faddp %st(0),%st(1)
fxch %st(3)
faddp %st(0),%st(5)
fxch %st(3)
faddp %st(0),%st(1)
fldl 72(%esp)
fmul %st(3),%st(0)
fldl 64(%esp)
fmul %st(4),%st(0)
fldl 56(%esp)
fmul %st(5),%st(0)
fldl 48(%esp)
fmulp %st(0),%st(6)
fldl 64(%esp)
fmul %st(5),%st(0)
faddp %st(0),%st(3)
fldl 56(%esp)
fmul %st(5),%st(0)
faddp %st(0),%st(2)
fldl 48(%esp)
fmul %st(5),%st(0)
faddp %st(0),%st(1)
fldl 96(%esp)
fmulp %st(0),%st(5)
fxch %st(4)
faddp %st(0),%st(5)
fldl 56(%esp)
fmul %st(6),%st(0)
faddp %st(0),%st(2)
fldl 48(%esp)
fmul %st(6),%st(0)
faddp %st(0),%st(1)
fldl 96(%esp)
fmul %st(6),%st(0)
faddp %st(0),%st(4)
fldl 88(%esp)
fmulp %st(0),%st(6)
fxch %st(5)
faddp %st(0),%st(4)
fldl 48(%esp)
fmul %st(2),%st(0)
faddp %st(0),%st(1)
fldl 96(%esp)
fmul %st(2),%st(0)
faddp %st(0),%st(5)
fldl 88(%esp)
fmul %st(2),%st(0)
faddp %st(0),%st(3)
fldl 80(%esp)
fmulp %st(0),%st(2)
fxch %st(1)
faddp %st(0),%st(3)
fxch %st(3)
fxch %st(2)
._nomorebytes:
fldl 0+poly1305_ppro_alpha130
fadd %st(4),%st(0)
fsubl 0+poly1305_ppro_alpha130
fsubr %st(0),%st(4)
fmull 0+poly1305_ppro_scale
fldl 0+poly1305_ppro_alpha32
fadd %st(2),%st(0)
fsubl 0+poly1305_ppro_alpha32
fsubr %st(0),%st(2)
fldl 0+poly1305_ppro_alpha64
fadd %st(4),%st(0)
fsubl 0+poly1305_ppro_alpha64
fsubr %st(0),%st(4)
fldl 0+poly1305_ppro_alpha96
fadd %st(6),%st(0)
fsubl 0+poly1305_ppro_alpha96
fsubr %st(0),%st(6)
fxch %st(4)
faddp %st(0),%st(3)
fxch %st(4)
faddp %st(0),%st(1)
fxch %st(4)
faddp %st(0),%st(3)
fxch %st(4)
faddp %st(0),%st(1)
fxch %st(3)
faddl 0+poly1305_ppro_hoffset0
fxch %st(2)
faddl 0+poly1305_ppro_hoffset1
fxch %st(1)
faddl 0+poly1305_ppro_hoffset2
fxch %st(3)
faddl 0+poly1305_ppro_hoffset3
fxch %st(2)
fstpl 16(%esp)
fstpl 24(%esp)
fxch %st(1)
fstpl 32(%esp)
fstpl 40(%esp)
movl 20(%esp),%ecx
and $63,%ecx
movl 28(%esp),%edx
and $63,%edx
movl 36(%esp),%ebx
and $63,%ebx
movl 44(%esp),%esi
and $63,%esi
movl 24(%esp),%eax
add %ecx,%eax
movl %eax,124(%esp)
movl 32(%esp),%eax
adc %edx,%eax
movl %eax,128(%esp)
movl 40(%esp),%eax
adc %ebx,%eax
movl %eax,132(%esp)
mov $0,%eax
adc %esi,%eax
movl %eax,136(%esp)
mov $5,%eax
movl 16(%esp),%edx
add %edx,%eax
movl %eax,140(%esp)
mov $0,%eax
movl 124(%esp),%ebx
adc %ebx,%eax
movl %eax,124(%esp)
mov $0,%eax
movl 128(%esp),%esi
adc %esi,%eax
movl %eax,128(%esp)
mov $0,%eax
movl 132(%esp),%edi
adc %edi,%eax
movl %eax,132(%esp)
mov $-4,%ecx
movl 136(%esp),%eax
adc %eax,%ecx
sar $16,%ecx
mov %ecx,%ebp
xor $-1,%ebp
and %ecx,%edx
movl 140(%esp),%eax
and %ebp,%eax
or %eax,%edx
and %ecx,%ebx
movl 124(%esp),%eax
and %ebp,%eax
or %eax,%ebx
and %ecx,%esi
movl 128(%esp),%eax
and %ebp,%eax
or %eax,%esi
and %ecx,%edi
movl 132(%esp),%eax
and %ebp,%eax
or %eax,%edi
movl 120(%esp),%eax
movl 12(%esp,%eax),%ecx
addl 0(%ecx),%edx
adcl 4(%ecx),%ebx
adcl 8(%ecx),%esi
adcl 12(%ecx),%edi
movl 4(%esp,%eax),%ecx
movl %edx,0(%ecx)
movl %ebx,4(%ecx)
movl %esi,8(%ecx)
movl %edi,12(%ecx)
movl 104(%esp),%ebx
movl 108(%esp),%esi
movl 112(%esp),%edi
movl 116(%esp),%ebp
add %eax,%esp
ret
